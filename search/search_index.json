{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Matfree: Matrix-free linear algebra in JAX","text":"<p>Randomised and deterministic matrix-free methods for trace estimation, functions of matrices, and matrix factorisations. Matfree builds on JAX.</p> <ul> <li>\u26a1 Stochastic trace estimation including batching, control variates, and uncertainty quantification</li> <li>\u26a1 A stand-alone implementation of stochastic Lanczos quadrature for traces of functions of matrices</li> <li>\u26a1 Matrix-decomposition algorithms for large sparse eigenvalue problems: tridiagonalisation, bidiagonalisation, and Hessenberg factorisation via Lanczos and Arnoldi iterations</li> <li>\u26a1 Chebyshev, Lanczos, and Arnoldi-based methods for approximating functions of large matrices</li> <li>\u26a1 Gradients of functions of large matrices (like in this paper) via differentiable Lanczos and Arnoldi iterations</li> <li>\u26a1 Partial Cholesky preconditioners with and without pivoting</li> <li>\u26a1 And many other things</li> </ul> <p>Everything is natively compatible with the rest of JAX: JIT compilation, automatic differentiation, vectorisation, and PyTrees. Let us know what you think about Matfree!</p> <p>Installation</p> <p>To install the package, run</p> <pre><code>pip install matfree\n</code></pre> <p>Important: This assumes you already have a working installation of JAX. To install JAX, follow these instructions. To combine Matfree with a CPU version of JAX, run</p> <pre><code>pip install matfree[cpu]\n</code></pre> <p>which is equivalent to combining <code>pip install jax[cpu]</code> with <code>pip install matfree</code>. (But do not only use Matfree on CPU!)</p> <p>Minimal example</p> <p>Import Matfree and JAX, and set up a test problem.</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from matfree import stochtrace\n&gt;&gt;&gt;\n&gt;&gt;&gt; A = jnp.reshape(jnp.arange(12.0), (6, 2))\n&gt;&gt;&gt;\n&gt;&gt;&gt; def matvec(x):\n...     return A.T @ (A @ x)\n...\n</code></pre> <p>Estimate the trace of the matrix:</p> <pre><code>&gt;&gt;&gt; # Determine the shape of the base-samples\n&gt;&gt;&gt; input_like = jnp.zeros((2,), dtype=float)\n&gt;&gt;&gt; sampler = stochtrace.sampler_rademacher(input_like, num=10_000)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Set Hutchinson's method up to compute the traces\n&gt;&gt;&gt; # (instead of, e.g., diagonals)\n&gt;&gt;&gt; integrand = stochtrace.integrand_trace()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute an estimator\n&gt;&gt;&gt; estimate = stochtrace.estimator(integrand, sampler)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Estimate\n&gt;&gt;&gt; key = jax.random.PRNGKey(1)\n&gt;&gt;&gt; trace = estimate(matvec, key)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(trace)\n504.0\n&gt;&gt;&gt;\n&gt;&gt;&gt; # for comparison:\n&gt;&gt;&gt; print((jnp.trace(A.T @ A)))\n506.0\n</code></pre> <p>Tutorials</p> <p>Find many more tutorials in Matfree's documentation.</p> <p>These tutorials include, among other things:</p> <ul> <li>Log-determinants: Use stochastic Lanczos quadrature to compute matrix functions.</li> <li>Pytree-valued states: Combining neural-network Jacobians with stochastic Lanczos quadrature.</li> <li>Control variates: Use control variates and multilevel schemes to reduce variances.</li> <li>Higher moments and UQ: Compute means, variances, and other moments simultaneously.</li> <li>Vector calculus: Use matrix-free linear algebra to implement vector calculus.</li> <li>Low-memory trace estimation: Combine Matfree's API with JAX's function transformations for low-memory stochastic trace estimation.</li> </ul> <p>Let us know what you use Matfree for!</p> <p>Citation</p> <p>Thank you for using Matfree! A dedicated paper about Matfree itself is in preparation.\\ In the meantime, if you are using Matfree's differentiable Lanczos or Arnoldi iterations, then you are using the algorithms introduced by this paper. We would appreciate it if you cited it as follows:</p> <pre><code>@article{kraemer2024gradients,\n  title={Gradients of functions of large matrices},\n  author={Kr{\\\"a}mer, Nicholas and Moreno-Mu{\\~n}oz, Pablo and Roy, Hrittik and Hauberg, S{\\o}ren},\n  journal={Advances in Neural Information Processing Systems},\n  volume={37},\n  pages={49484--49518},\n  year={2024}\n}\n</code></pre> <p>If you are using Matfree's differentiable LSMR implementation, then you are using the algorithm from this paper. We would appreciate it if you cited it as follows:</p> <pre><code>@article{roy2025matrix,\n    title={Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them},\n    author={Roy, Hrittik and Hauberg, S{\\\\o}ren and Kr{\\\"a}mer, Nicholas},\n    journal={arXiv preprint arXiv:2510.19634},\n    year={2025}\n}\n</code></pre> <p>Some of Matfree's docstrings contain additional bibliographic information. For example, the <code>matfree.bounds</code> functions link to bibtex entries for the articles associated with each bound. Go check out the API documentation.</p>"},{"location":"API_documentation/bounds/","title":"matfree.bounds","text":""},{"location":"API_documentation/bounds/#matfree.bounds","title":"<code>matfree.bounds</code>","text":"<p>Matrix-free bounds on functions of matrices.</p>"},{"location":"API_documentation/bounds/#matfree.bounds.baigolub96_logdet_spd","title":"<code>matfree.bounds.baigolub96_logdet_spd(bound_spectrum, /, nrows, trace, norm_frobenius_squared)</code>","text":"<p>Bound the log-determinant of a symmetric, positive definite matrix.</p> <p>This function implements Theorem 2 in the paper by Bai and Golub (1996).</p> <p><code>bound_spectrum</code> is either an upper or a lower bound on the spectrum of the matrix. If it is an upper bound, the function returns an upper bound of the log-determinant. If it is a lower bound, the function returns a lower bound of the log-determinant.</p> BibTex for Bai and Golub (1996) <pre><code>@article{bai1996bounds,\n    title={Bounds for the trace of the inverse and the\n    determinant of symmetric positive definite matrices},\n    author={Bai, Zhaojun and Golub, Gene H},\n    journal={Annals of Numerical Mathematics},\n    volume={4},\n    pages={29--38},\n    year={1996},\n    publisher={Citeseer}\n}\n</code></pre> Source code in <code>matfree/bounds.py</code> <pre><code>def baigolub96_logdet_spd(bound_spectrum, /, nrows, trace, norm_frobenius_squared):\n    \"\"\"Bound the log-determinant of a symmetric, positive definite matrix.\n\n    This function implements Theorem 2 in the paper by Bai and Golub (1996).\n\n    ``bound_spectrum`` is either an upper or a lower bound\n    on the spectrum of the matrix.\n    If it is an upper bound,\n    the function returns an upper bound of the log-determinant.\n    If it is a lower bound,\n    the function returns a lower bound of the log-determinant.\n\n\n\n    ??? note \"BibTex for Bai and Golub (1996)\"\n        ```bibtex\n        @article{bai1996bounds,\n            title={Bounds for the trace of the inverse and the\n            determinant of symmetric positive definite matrices},\n            author={Bai, Zhaojun and Golub, Gene H},\n            journal={Annals of Numerical Mathematics},\n            volume={4},\n            pages={29--38},\n            year={1996},\n            publisher={Citeseer}\n        }\n        ```\n    \"\"\"\n    mu1, mu2 = trace, norm_frobenius_squared\n    beta = bound_spectrum\n    tbar = (beta * mu1 - mu2) / (beta * nrows - mu1)\n    v = np.asarray([np.log(beta), np.log(tbar)])\n    w = np.asarray([mu1, mu2])\n    A = np.asarray([[beta, tbar], [beta**2, tbar**2]])\n    return v @ linalg.solve(A, w)\n</code></pre>"},{"location":"API_documentation/decomp/","title":"matfree.decomp","text":""},{"location":"API_documentation/decomp/#matfree.decomp","title":"<code>matfree.decomp</code>","text":"<p>Matrix-free matrix decompositions.</p> <p>This module includes various Lanczos-decompositions of matrices (tri-diagonal, bi-diagonal, etc.).</p> <p>For stochastic Lanczos quadrature and matrix-function-vector products, see matfree.funm.</p>"},{"location":"API_documentation/decomp/#matfree.decomp.bidiag","title":"<code>matfree.decomp.bidiag(num_matvecs: int, /, materialize: bool = True, reortho: str = 'full')</code>","text":"<p>Construct an implementation of bidiagonalisation.</p> <p>Uses pre-allocation and full reorthogonalisation.</p> <p>Works for arbitrary matrices. No symmetry required.</p> <p>Decompose a matrix into a product of orthogonal-bidiagonal-orthogonal matrices. Use this algorithm for approximate singular value decompositions.</p> <p>Internally, Matfree uses JAX to turn matrix-vector- into vector-matrix-products.</p> A note about differentiability <p>Unlike tridiag_sym or hessenberg, this function's reverse-mode derivatives are not efficient. Custom gradients for bidiagonalisation are a work in progress. In the meantime, if you need to differentiate the decompositions, consider using tridiag_sym instead (if possible).</p> Source code in <code>matfree/decomp.py</code> <pre><code>def bidiag(num_matvecs: int, /, materialize: bool = True, reortho: str = \"full\"):\n    \"\"\"Construct an implementation of **bidiagonalisation**.\n\n    Uses pre-allocation and full reorthogonalisation.\n\n    Works for **arbitrary matrices**. No symmetry required.\n\n    Decompose a matrix into a product of orthogonal-**bidiagonal**-orthogonal matrices.\n    Use this algorithm for approximate **singular value** decompositions.\n\n    Internally, Matfree uses JAX to turn matrix-vector- into vector-matrix-products.\n\n    ??? note \"A note about differentiability\"\n        Unlike [tridiag_sym][matfree.decomp.tridiag_sym] or\n        [hessenberg][matfree.decomp.hessenberg], this function's reverse-mode\n        derivatives are not efficient. Custom gradients for bidiagonalisation\n        are a work in progress. In the meantime,\n        if you need to differentiate the decompositions, consider using\n        [tridiag_sym][matfree.decomp.tridiag_sym] instead (if possible).\n\n    \"\"\"\n\n    def estimate(Av: Callable, v0, *parameters):\n        # Infer the size of A from v0\n        (ncols,) = np.shape(v0)\n        w0_like = func.eval_shape(Av, v0, *parameters)\n        (nrows,) = np.shape(w0_like)\n\n        # Complain if the shapes don't match\n        max_num_matvecs = min(nrows, ncols)\n        if num_matvecs &gt; max_num_matvecs or num_matvecs &lt; 0:\n            msg = _error_num_matvecs(num_matvecs, maxval=min(nrows, ncols), minval=0)\n            raise ValueError(msg)\n\n        v0_norm, length = _normalise(v0)\n        init_val = init(v0_norm, nrows=nrows, ncols=ncols)\n\n        if num_matvecs == 0:\n            uk_all_T, J, vk_all, (beta, vk) = extract(init_val)\n            return _DecompResult(\n                Q_tall=(uk_all_T, vk_all.T),\n                J_small=J,\n                residual=beta * vk,\n                init_length_inv=1 / length,\n            )\n\n        def body_fun(_, s):\n            return step(Av, s, *parameters)\n\n        result = control_flow.fori_loop(\n            0, num_matvecs, body_fun=body_fun, init_val=init_val\n        )\n        uk_all_T, J, vk_all, (beta, vk) = extract(result)\n        return _DecompResult(\n            Q_tall=(uk_all_T, vk_all.T),\n            J_small=J,\n            residual=beta * vk,\n            init_length_inv=1 / length,\n        )\n\n    class State(containers.NamedTuple):\n        i: int\n        Us: Array\n        Vs: Array\n        alphas: Array\n        betas: Array\n        beta: Array\n        vk: Array\n\n    def init(init_vec: Array, *, nrows, ncols) -&gt; State:\n        alphas = np.zeros((num_matvecs,))\n        betas = np.zeros((num_matvecs,))\n        Us = np.zeros((num_matvecs, nrows))\n        Vs = np.zeros((num_matvecs, ncols))\n        v0, _ = _normalise(init_vec)\n        return State(0, Us, Vs, alphas, betas, np.zeros(()), v0)\n\n    def step(Av, state: State, *parameters) -&gt; State:\n        i, Us, Vs, alphas, betas, beta, vk = state\n        Vs = Vs.at[i].set(vk)\n        betas = betas.at[i].set(beta)\n\n        # Use jax.vjp to evaluate the vector-matrix product\n        Av_eval, vA = func.vjp(lambda v: Av(v, *parameters), vk)\n        uk = Av_eval - beta * Us[i - 1]\n        if reortho == \"full\":\n            # For some reason, two reorthogonalsiation calls are needed...\n            uk = uk - Us.T @ (Us @ uk)\n            uk = uk - Us.T @ (Us @ uk)\n\n        uk, alpha = _normalise(uk)\n        Us = Us.at[i].set(uk)\n        alphas = alphas.at[i].set(alpha)\n\n        (vA_eval,) = vA(uk)\n        vk = vA_eval - alpha * vk\n        if reortho == \"full\":\n            # For some reason, two reorthogonalsiation calls are needed...\n            vk = vk - Vs.T @ (Vs @ vk)\n            vk = vk - Vs.T @ (Vs @ vk)\n\n        vk, beta = _normalise(vk)\n\n        return State(i + 1, Us, Vs, alphas, betas, beta, vk)\n\n    def extract(state: State, /):\n        _, uk_all, vk_all, alphas, betas, beta, vk = state\n\n        if materialize:\n            B = _todense_bidiag(alphas, betas[1:])\n            return uk_all.T, B, vk_all, (beta, vk)\n\n        return uk_all.T, (alphas, betas[1:]), vk_all, (beta, vk)\n\n    def _normalise(vec):\n        length = linalg.vector_norm(vec)\n        return vec / length, length\n\n    def _todense_bidiag(d, e):\n        diag = linalg.diagonal_matrix(d)\n        offdiag = linalg.diagonal_matrix(e, 1)\n        return diag + offdiag\n\n    return estimate\n</code></pre>"},{"location":"API_documentation/decomp/#matfree.decomp.hessenberg","title":"<code>matfree.decomp.hessenberg(num_matvecs, /, *, reortho: str, custom_vjp: bool = True, reortho_vjp: str = 'match')</code>","text":"<p>Construct a Hessenberg-factorisation via the Arnoldi iteration.</p> <p>Uses pre-allocation, and full reorthogonalisation if <code>reortho</code> is set to <code>\"full\"</code>. It tends to be a good idea to use full reorthogonalisation.</p> <p>This algorithm works for arbitrary matrices.</p> <p>Setting <code>custom_vjp</code> to <code>True</code> implies using efficient, numerically stable gradients of the Arnoldi iteration according to what has been proposed by Kr\u00e4mer et al. (2024). These gradients are exact, so there is little reason not to use them. If you use this configuration, please consider citing Kr\u00e4mer et al. (2024; bibtex below).</p> BibTex for Kr\u00e4mer et al. (2024) <pre><code>@article{kraemer2024gradients,\n    title={Gradients of functions of large matrices},\n    author={Kr\\\"amer, Nicholas and Moreno-Mu\\~noz, Pablo and\n    Roy, Hrittik and Hauberg, S{\\o}ren},\n    journal={arXiv preprint arXiv:2405.17277},\n    year={2024}\n}\n</code></pre> Source code in <code>matfree/decomp.py</code> <pre><code>def hessenberg(\n    num_matvecs, /, *, reortho: str, custom_vjp: bool = True, reortho_vjp: str = \"match\"\n):\n    r\"\"\"Construct a **Hessenberg-factorisation** via the Arnoldi iteration.\n\n    Uses pre-allocation, and full reorthogonalisation if `reortho` is set to `\"full\"`.\n    It tends to be a good idea to use full reorthogonalisation.\n\n    This algorithm works for **arbitrary matrices**.\n\n    Setting `custom_vjp` to `True` implies using efficient, numerically stable\n    gradients of the Arnoldi iteration according to what has been proposed by\n    Kr\u00e4mer et al. (2024).\n    These gradients are exact, so there is little reason not to use them.\n    If you use this configuration,\n    please consider citing Kr\u00e4mer et al. (2024; bibtex below).\n\n    ??? note \"BibTex for Kr\u00e4mer et al. (2024)\"\n        ```bibtex\n        @article{kraemer2024gradients,\n            title={Gradients of functions of large matrices},\n            author={Kr\\\"amer, Nicholas and Moreno-Mu\\~noz, Pablo and\n            Roy, Hrittik and Hauberg, S{\\o}ren},\n            journal={arXiv preprint arXiv:2405.17277},\n            year={2024}\n        }\n        ```\n    \"\"\"\n    reortho_expected = [\"none\", \"full\"]\n    if reortho not in reortho_expected:\n        msg = f\"Unexpected input for {reortho}: either of {reortho_expected} expected.\"\n        raise TypeError(msg)\n\n    def estimate(matvec, v, *params):\n        matvec_convert, aux_args = func.closure_convert(matvec, v, *params)\n        return _estimate(matvec_convert, v, *params, *aux_args)\n\n    def _estimate(matvec_convert: Callable, v, *params):\n        return _hessenberg_forward(\n            matvec_convert, num_matvecs, v, *params, reortho=reortho_vjp\n        )\n\n    def estimate_fwd(matvec_convert: Callable, v, *params):\n        outputs = _estimate(matvec_convert, v, *params)\n        return outputs, (outputs, params)\n\n    def estimate_bwd(matvec_convert: Callable, cache, vjp_incoming):\n        (Q, H, r, c), params = cache\n        dQ, dH, dr, dc = vjp_incoming\n\n        return _hessenberg_adjoint(\n            matvec_convert,\n            *params,\n            Q=Q,\n            H=H,\n            r=r,\n            c=c,\n            dQ=dQ,\n            dH=dH,\n            dr=dr,\n            dc=dc,\n            reortho=reortho,\n        )\n\n    if custom_vjp:\n        _estimate = func.custom_vjp(_estimate, nondiff_argnums=(0,))\n        _estimate.defvjp(estimate_fwd, estimate_bwd)  # type: ignore\n    return estimate\n</code></pre>"},{"location":"API_documentation/decomp/#matfree.decomp.tridiag_sym","title":"<code>matfree.decomp.tridiag_sym(num_matvecs: int, /, *, materialize: bool = True, reortho: str = 'full', custom_vjp: bool = True)</code>","text":"<p>Construct an implementation of tridiagonalisation.</p> <p>Decompose a symmetric matrix into a product of orthogonal-tridiagonal-orthogonal matrices. Use this algorithm for approximate eigenvalue decompositions. The present implementation allocates all Lanczos vectors before running the algorithm. If <code>reortho</code> is set to <code>\"full\"</code>, it also uses full reorthogonalisation. It is usually a good idea to use full reorthogonalisation. Matrix-free tridiagonalisation uses Lanczos' (1950) algorithm:</p> BibTex for Lanczos (1950) <pre><code>@article{lanczos1950iteration,\n    title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},\n    author={Lanczos, Cornelius},\n    journal={Journal of research of the National Bureau of Standards},\n    volume={45},\n    number={4},\n    pages={255--282},\n    year={1950}\n}\n</code></pre> <p>Setting <code>custom_vjp</code> to <code>True</code> implies using efficient, numerically stable gradients of the Lanczos iteration which was proposed by Kr\u00e4mer et al. (2024). These gradients are exact, so there is little reason not to use them. If you use this configuration, please cite Kr\u00e4mer et al. (2024):</p> BibTex for Kr\u00e4mer et al. (2024) <pre><code>@article{kraemer2024gradients,\n    title={Gradients of functions of large matrices},\n    author={Kr{\\\"a}mer, Nicholas and Moreno-Mu{\\~n}oz, Pablo and Roy, Hrittik and Hauberg, S{\\o}ren},\n    journal={Advances in Neural Information Processing Systems},\n    volume={37},\n    pages={49484--49518},\n    year={2024}\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_matvecs</code> <code>int</code> <p>The number of matrix-vector products aka the depth of the Krylov space. The deeper the Krylov space, the more accurate the factorisation tends to be. However, the computational complexity increases linearly with the number of matrix-vector products.</p> required <code>materialize</code> <code>bool</code> <p>The value of this flag indicates whether the tridiagonal matrix should be returned in a sparse format (which means, as a tuple of diagonas) or as a dense matrix. The dense matrix is helpful if different decompositions should be used interchangeably. The sparse representation requires less memory.</p> <code>True</code> <code>reortho</code> <code>str</code> <p>The value of this parameter indicates whether to reorthogonalise the basis vectors during the forward pass. Reorthogonalisation makes the forward pass more expensive, but helps (significantly) with numerical stability.</p> <code>'full'</code> <code>custom_vjp</code> <code>bool</code> <p>The value of this flag indicates whether to use a custom vector-Jacobian product as proposed by Kr\u00e4mer et al. (2024; bibtex above). Generally, using a custom VJP tends to be a good idea. However, due to JAX's mechanics, a custom VJP precludes the use of forward-mode differentiation (see here), so don't use a custom VJP if you need forward-mode differentiation.</p> <code>True</code> <p>Returns:</p> Type Description <code>decompose</code> <p>A decomposition function that maps <code>(matvec, vector, *params)</code> to the decomposition. The decomposition is a tuple of (nested) arrays. The first element is the Krylov basis, the second element represents the tridiagonal matrix (how it is represented depends on the value of ``materialize''), the third element is the residual, and the fourth element is the (inverse of the) length of the initial vector.</p> Source code in <code>matfree/decomp.py</code> <pre><code>def tridiag_sym(\n    num_matvecs: int,\n    /,\n    *,\n    materialize: bool = True,\n    reortho: str = \"full\",\n    custom_vjp: bool = True,\n):\n    r\"\"\"Construct an implementation of **tridiagonalisation**.\n\n    Decompose a **symmetric** matrix into a product of orthogonal-**tridiagonal**-orthogonal matrices.\n    Use this algorithm for approximate **eigenvalue** decompositions.\n    The present implementation allocates all Lanczos vectors before running the\n    algorithm. If `reortho` is set to `\"full\"`, it also uses full reorthogonalisation.\n    It is usually a good idea to use full reorthogonalisation.\n    Matrix-free tridiagonalisation uses Lanczos' (1950) algorithm:\n\n    ??? note \"BibTex for Lanczos (1950)\"\n        ```bibtex\n        @article{lanczos1950iteration,\n            title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},\n            author={Lanczos, Cornelius},\n            journal={Journal of research of the National Bureau of Standards},\n            volume={45},\n            number={4},\n            pages={255--282},\n            year={1950}\n        }\n        ```\n\n    Setting `custom_vjp` to `True` implies using efficient, numerically stable\n    gradients of the Lanczos iteration which was proposed by Kr\u00e4mer et al. (2024).\n    These gradients are exact, so there is little reason not to use them.\n    If you use this configuration, please cite Kr\u00e4mer et al. (2024):\n\n    ??? note \"BibTex for Kr\u00e4mer et al. (2024)\"\n        ```bibtex\n        @article{kraemer2024gradients,\n            title={Gradients of functions of large matrices},\n            author={Kr{\\\"a}mer, Nicholas and Moreno-Mu{\\~n}oz, Pablo and Roy, Hrittik and Hauberg, S{\\o}ren},\n            journal={Advances in Neural Information Processing Systems},\n            volume={37},\n            pages={49484--49518},\n            year={2024}\n        }\n        ```\n\n    Parameters\n    ----------\n    num_matvecs\n        The number of matrix-vector products aka the depth of the Krylov space.\n        The deeper the Krylov space, the more accurate the factorisation tends to be.\n        However, the computational complexity increases linearly\n        with the number of matrix-vector products.\n    materialize\n        The value of this flag indicates whether the tridiagonal matrix\n        should be returned in a sparse format (which means, as a tuple of diagonas)\n        or as a dense matrix.\n        The dense matrix is helpful if different decompositions should be used\n        interchangeably. The sparse representation requires less memory.\n    reortho\n        The value of this parameter indicates whether to reorthogonalise the\n        basis vectors during the forward pass.\n        Reorthogonalisation makes the forward pass more expensive, but helps\n        (significantly) with numerical stability.\n    custom_vjp\n        The value of this flag indicates whether to use a custom vector-Jacobian\n        product as proposed by Kr\u00e4mer et al. (2024; bibtex above).\n        Generally, using a custom VJP tends to be a good idea.\n        However, due to JAX's mechanics, a custom VJP precludes the use of forward-mode\n        differentiation\n        ([see here](https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_vjp.html)),\n        so don't use a custom VJP if you need forward-mode differentiation.\n\n    Returns\n    -------\n    decompose\n        A decomposition function that maps\n        ``(matvec, vector, *params)`` to the decomposition.\n        The decomposition is a tuple of (nested) arrays.\n        The first element is the Krylov basis,\n        the second element represents the tridiagonal matrix\n        (how it is represented depends on the value of ``materialize''),\n        the third element is\n        the residual, and the fourth element is\n        the (inverse of the) length of the initial vector.\n    \"\"\"\n    if reortho == \"full\":\n        return _tridiag_reortho_full(\n            num_matvecs, custom_vjp=custom_vjp, materialize=materialize\n        )\n    if reortho == \"none\":\n        return _tridiag_reortho_none(\n            num_matvecs, custom_vjp=custom_vjp, materialize=materialize\n        )\n\n    msg = f\"reortho={reortho} unsupported. Choose eiter {'full', 'none'}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"API_documentation/eig/","title":"matfree.eig","text":""},{"location":"API_documentation/eig/#matfree.eig","title":"<code>matfree.eig</code>","text":"<p>Matrix-free eigenvalue and singular-value analysis.</p>"},{"location":"API_documentation/eig/#matfree.eig.eig_partial","title":"<code>matfree.eig.eig_partial(hessenberg: Callable) -&gt; Callable</code>","text":"<p>Partial eigenvalue decomposition.</p> <p>Combines Hessenberg factorisation with a decomposition of the (small) Hessenberg matrix.</p> <p>Parameters:</p> Name Type Description Default <code>hessenberg</code> <code>Callable</code> <p>An implementation of Hessenberg factorisation. For example, the output of decomp.hessenberg.</p> required Source code in <code>matfree/eig.py</code> <pre><code>def eig_partial(hessenberg: Callable) -&gt; Callable:\n    \"\"\"Partial eigenvalue decomposition.\n\n    Combines Hessenberg factorisation with a decomposition\n    of the (small) Hessenberg matrix.\n\n    Parameters\n    ----------\n    hessenberg:\n        An implementation of Hessenberg factorisation.\n        For example, the output of\n        [decomp.hessenberg][matfree.decomp.hessenberg].\n\n    \"\"\"\n\n    def eig(Av: Callable, v0: Array, *parameters):\n        # Flatten in- and outputs\n        Av_flat, flattened = _partial_and_flatten_matvec(Av, v0, *parameters)\n        _, (v0_flat, v_unravel) = flattened\n\n        # Call the flattened eig\n        vals, vecs = eig_flat(Av_flat, v0_flat)\n\n        # Unravel the eigenvectors\n        vecs = func.vmap(v_unravel)(vecs)\n        return vals, vecs\n\n    def eig_flat(Av: Callable, v0: Array):\n        # Factorise the matrix\n        Q, H, *_ = hessenberg(Av, v0)\n\n        # Compute eig of factorisation\n        vals, vecs = linalg.eig(H)\n        vecs = Q @ vecs\n        return vals, vecs.T\n\n    return eig\n</code></pre>"},{"location":"API_documentation/eig/#matfree.eig.eigh_partial","title":"<code>matfree.eig.eigh_partial(tridiag_sym: Callable) -&gt; Callable</code>","text":"<p>Partial symmetric/Hermitian eigenvalue decomposition.</p> <p>Combines tridiagonalization with a decomposition of the (small) tridiagonal matrix.</p> <p>Parameters:</p> Name Type Description Default <code>tridiag_sym</code> <code>Callable</code> <p>An implementation of tridiagonalization. For example, the output of decomp.tridiag_sym.</p> required Source code in <code>matfree/eig.py</code> <pre><code>def eigh_partial(tridiag_sym: Callable) -&gt; Callable:\n    \"\"\"Partial symmetric/Hermitian eigenvalue decomposition.\n\n    Combines tridiagonalization with a decomposition\n    of the (small) tridiagonal matrix.\n\n    Parameters\n    ----------\n    tridiag_sym:\n        An implementation of tridiagonalization.\n        For example, the output of\n        [decomp.tridiag_sym][matfree.decomp.tridiag_sym].\n\n    \"\"\"\n\n    def eigh(Av: Callable, v0: Array, *parameters):\n        # Flatten in- and outputs\n        Av_flat, flattened = _partial_and_flatten_matvec(Av, v0, *parameters)\n        _, (v0_flat, v_unravel) = flattened\n\n        # Call the flattened eigh\n        vals, vecs = eigh_flat(Av_flat, v0_flat)\n\n        # Unravel the eigenvectors\n        vecs = func.vmap(v_unravel)(vecs)\n        return vals, vecs\n\n    def eigh_flat(Av: Callable, v0: Array):\n        # Factorise the matrix\n        Q, H, *_ = tridiag_sym(Av, v0)\n\n        # Compute eigh of factorisation\n        vals, vecs = linalg.eigh(H)\n        vecs = Q @ vecs\n        return vals, vecs.T\n\n    return eigh\n</code></pre>"},{"location":"API_documentation/eig/#matfree.eig.svd_partial","title":"<code>matfree.eig.svd_partial(bidiag: Callable) -&gt; Callable</code>","text":"<p>Partial singular value decomposition.</p> <p>Combines bidiagonalisation with a full SVD of the (small) bidiagonal matrix.</p> <p>Parameters:</p> Name Type Description Default <code>bidiag</code> <code>Callable</code> <p>An implementation of bidiagonalisation. For example, the output of decomp.bidiag. Note how this function assumes that the bidiagonalisation materialises the bidiagonal matrix.</p> required Source code in <code>matfree/eig.py</code> <pre><code>def svd_partial(bidiag: Callable) -&gt; Callable:\n    \"\"\"Partial singular value decomposition.\n\n    Combines bidiagonalisation with a full SVD of the (small) bidiagonal matrix.\n\n    Parameters\n    ----------\n    bidiag:\n        An implementation of bidiagonalisation.\n        For example, the output of\n        [decomp.bidiag][matfree.decomp.bidiag].\n        Note how this function assumes that the bidiagonalisation\n        materialises the bidiagonal matrix.\n\n    \"\"\"\n\n    def svd(Av: Callable, v0: Array, *parameters):\n        # Flatten in- and outputs\n        Av_flat, flattened = _partial_and_flatten_matvec(Av, v0, *parameters)\n        (_u0_flat, u_unravel), (v0_flat, v_unravel) = flattened\n\n        # Call the flattened SVD\n        ut, s, vt = svd_flat(Av_flat, v0_flat)\n\n        # Unravel the singular vectors\n        ut_tree = func.vmap(u_unravel)(ut)\n        vt_tree = func.vmap(v_unravel)(vt)\n        return ut_tree, s, vt_tree\n\n    def svd_flat(Av: Callable, v0: Array):\n        # Factorise the matrix\n        (u, v), B, *_ = bidiag(Av, v0)\n\n        # Compute SVD of factorisation\n        U, S, Vt = linalg.svd(B, full_matrices=False)\n\n        # Combine orthogonal transformations\n        return (u @ U).T, S, Vt @ v.T\n\n    return svd\n</code></pre>"},{"location":"API_documentation/funm/","title":"matfree.funm","text":""},{"location":"API_documentation/funm/#matfree.funm","title":"<code>matfree.funm</code>","text":"<p>Matrix-free implementations of functions of matrices.</p> <p>This includes matrix-function-vector products</p> \\[ (f, A, v, p) \\mapsto f(A(p))v \\] <p>as well as matrix-function extensions for stochastic trace estimation, which provide</p> \\[ (f, A, v, p) \\mapsto v^\\top f(A(p))v. \\] <p>Plug these integrands into matfree.stochtrace.estimator.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.random\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from matfree import decomp\n&gt;&gt;&gt;\n&gt;&gt;&gt; M = jax.random.normal(jax.random.PRNGKey(1), shape=(10, 10))\n&gt;&gt;&gt; A = M.T @ M\n&gt;&gt;&gt; v = jax.random.normal(jax.random.PRNGKey(2), shape=(10,))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Compute a matrix-logarithm with Lanczos' algorithm\n&gt;&gt;&gt; matfun = dense_funm_sym_eigh(jnp.log)\n&gt;&gt;&gt; tridiag = decomp.tridiag_sym(4)\n&gt;&gt;&gt; matfun_vec = funm_lanczos_sym(matfun, tridiag)\n&gt;&gt;&gt; fAx = matfun_vec(lambda s: A @ s, v)\n&gt;&gt;&gt; print(fAx.shape)\n(10,)\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.dense_funm_pade_exp","title":"<code>matfree.funm.dense_funm_pade_exp()</code>","text":"<p>Implement dense matrix-exponentials using a Pade approximation.</p> <p>Use it to construct one of the matrix-free matrix-function implementations, e.g. matfree.funm.funm_arnoldi.</p> Source code in <code>matfree/funm.py</code> <pre><code>def dense_funm_pade_exp():\n    \"\"\"Implement dense matrix-exponentials using a Pade approximation.\n\n    Use it to construct one of the matrix-free matrix-function implementations,\n    e.g. [matfree.funm.funm_arnoldi][matfree.funm.funm_arnoldi].\n    \"\"\"\n\n    def fun(dense_matrix):\n        return linalg.funm_pade_exp(dense_matrix)\n\n    return fun\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.dense_funm_product_svd","title":"<code>matfree.funm.dense_funm_product_svd(matfun)</code>","text":"<p>Implement dense matrix-functions of a product of matrices via SVDs.</p> Source code in <code>matfree/funm.py</code> <pre><code>def dense_funm_product_svd(matfun):\n    \"\"\"Implement dense matrix-functions of a product of matrices via SVDs.\"\"\"\n\n    def dense_funm(matrix, /):\n        # Compute SVD of factorisation\n        _, S, Vt = linalg.svd(matrix, full_matrices=False)\n\n        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n        eigvals, eigvecs = S**2, Vt.T\n        fx_eigvals = func.vmap(matfun)(eigvals)\n        return eigvecs @ (fx_eigvals[:, None] * eigvecs.T)\n\n    return dense_funm\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.dense_funm_schur","title":"<code>matfree.funm.dense_funm_schur(matfun)</code>","text":"<p>Implement dense matrix-functions via symmetric Schur decompositions.</p> <p>Use it to construct one of the matrix-free matrix-function implementations, e.g. matfree.funm.funm_lanczos_sym.</p> Source code in <code>matfree/funm.py</code> <pre><code>def dense_funm_schur(matfun):\n    \"\"\"Implement dense matrix-functions via symmetric Schur decompositions.\n\n    Use it to construct one of the matrix-free matrix-function implementations,\n    e.g. [matfree.funm.funm_lanczos_sym][matfree.funm.funm_lanczos_sym].\n    \"\"\"\n\n    def fun(dense_matrix):\n        return linalg.funm_schur(dense_matrix, matfun)\n\n    return fun\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.dense_funm_sym_eigh","title":"<code>matfree.funm.dense_funm_sym_eigh(matfun)</code>","text":"<p>Implement dense matrix-functions via symmetric eigendecompositions.</p> <p>Use it to construct one of the matrix-free matrix-function implementations, e.g. matfree.funm.funm_lanczos_sym.</p> Source code in <code>matfree/funm.py</code> <pre><code>def dense_funm_sym_eigh(matfun):\n    \"\"\"Implement dense matrix-functions via symmetric eigendecompositions.\n\n    Use it to construct one of the matrix-free matrix-function implementations,\n    e.g. [matfree.funm.funm_lanczos_sym][matfree.funm.funm_lanczos_sym].\n    \"\"\"\n\n    def fun(dense_matrix):\n        eigvals, eigvecs = linalg.eigh(dense_matrix)\n        fx_eigvals = func.vmap(matfun)(eigvals)\n        return eigvecs @ linalg.diagonal(fx_eigvals) @ eigvecs.T\n\n    return fun\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.funm_arnoldi","title":"<code>matfree.funm.funm_arnoldi(dense_funm: Callable, hessenberg: Callable) -&gt; Callable</code>","text":"<p>Implement a matrix-function-vector product via the Arnoldi iteration.</p> <p>This algorithm uses the Arnoldi iteration and therefore applies only to all square matrices.</p> <p>Parameters:</p> Name Type Description Default <code>dense_funm</code> <code>Callable</code> <p>An implementation of a function of a dense matrix. For example, the output of funm.dense_funm_sym_eigh funm.dense_funm_schur</p> required <code>hessenberg</code> <code>Callable</code> <p>An implementation of Hessenberg-factorisation. E.g., the output of decomp.hessenberg.</p> required Source code in <code>matfree/funm.py</code> <pre><code>def funm_arnoldi(dense_funm: Callable, hessenberg: Callable, /) -&gt; Callable:\n    \"\"\"Implement a matrix-function-vector product via the Arnoldi iteration.\n\n    This algorithm uses the Arnoldi iteration\n    and therefore applies only to all square matrices.\n\n    Parameters\n    ----------\n    dense_funm\n        An implementation of a function of a dense matrix.\n        For example, the output of\n        [funm.dense_funm_sym_eigh][matfree.funm.dense_funm_sym_eigh]\n        [funm.dense_funm_schur][matfree.funm.dense_funm_schur]\n    hessenberg\n        An implementation of Hessenberg-factorisation.\n        E.g., the output of\n        [decomp.hessenberg][matfree.decomp.hessenberg].\n    \"\"\"\n\n    def estimate(matvec: Callable, vec, *parameters):\n        length = linalg.vector_norm(vec)\n        vec /= length\n        basis, matrix, *_ = hessenberg(matvec, vec, *parameters)\n\n        funm = dense_funm(matrix)\n        e1 = np.eye(len(matrix))[0, :]\n        return length * (basis @ funm @ e1)\n\n    return estimate\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.funm_chebyshev","title":"<code>matfree.funm.funm_chebyshev(matfun: Callable, num_matvecs: int, matvec: Callable) -&gt; Callable</code>","text":"<p>Compute a matrix-function-vector product via Chebyshev's algorithm.</p> <p>This function assumes that the spectrum of the matrix-vector product is contained in the interval (-1, 1), and that the matrix-function is analytic on this interval. If this is not the case, transform the matrix-vector product and the matrix-function accordingly.</p> Source code in <code>matfree/funm.py</code> <pre><code>def funm_chebyshev(matfun: Callable, num_matvecs: int, matvec: Callable, /) -&gt; Callable:\n    \"\"\"Compute a matrix-function-vector product via Chebyshev's algorithm.\n\n    This function assumes that the **spectrum of the matrix-vector product\n    is contained in the interval (-1, 1)**, and that the **matrix-function\n    is analytic on this interval**. If this is not the case,\n    transform the matrix-vector product and the matrix-function accordingly.\n    \"\"\"\n    # Construct nodes\n    nodes = _chebyshev_nodes(num_matvecs)\n    fx_nodes = matfun(nodes)\n\n    class _ChebyshevState(containers.NamedTuple):\n        interpolation: Array\n        poly_coefficients: tuple[Array, Array]\n        poly_values: tuple[Array, Array]\n\n    def init_func(vec, *parameters):\n        # Initialize the scalar recursion\n        # (needed to compute the interpolation weights)\n        t2_n, t1_n = nodes, np.ones_like(nodes)\n        c1 = np.mean(fx_nodes * t1_n)\n        c2 = 2 * np.mean(fx_nodes * t2_n)\n\n        # Initialize the vector-valued recursion\n        # (this is where the matvec happens)\n        t2_x, t1_x = matvec(vec, *parameters), vec\n        value = c1 * t1_x + c2 * t2_x\n        return _ChebyshevState(value, (t2_n, t1_n), (t2_x, t1_x))\n\n    def recursion_func(val: _ChebyshevState, *parameters) -&gt; _ChebyshevState:\n        value, (t2_n, t1_n), (t2_x, t1_x) = val\n\n        # Apply the next scalar recursion and\n        # compute the next coefficient\n        t2_n, t1_n = 2 * nodes * t2_n - t1_n, t2_n\n        c2 = 2 * np.mean(fx_nodes * t2_n)\n\n        # Apply the next matrix-vector product recursion and\n        # compute the next interpolation-value\n        t2_x, t1_x = 2 * matvec(t2_x, *parameters) - t1_x, t2_x\n        value += c2 * t2_x\n        return _ChebyshevState(value, (t2_n, t1_n), (t2_x, t1_x))\n\n    def extract_func(val: _ChebyshevState):\n        return val.interpolation\n\n    alg = (0, num_matvecs - 1), init_func, recursion_func, extract_func\n    return _funm_polyexpand(alg)\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.funm_lanczos_sym","title":"<code>matfree.funm.funm_lanczos_sym(dense_funm: Callable, tridiag_sym: Callable) -&gt; Callable</code>","text":"<p>Implement a matrix-function-vector product via Lanczos' tridiagonalisation.</p> <p>This algorithm uses Lanczos' tridiagonalisation and therefore applies only to symmetric matrices.</p> <p>Parameters:</p> Name Type Description Default <code>dense_funm</code> <code>Callable</code> <p>An implementation of a function of a dense matrix. For example, the output of funm.dense_funm_sym_eigh funm.dense_funm_schur</p> required <code>tridiag_sym</code> <code>Callable</code> <p>An implementation of tridiagonalisation. E.g., the output of decomp.tridiag_sym.</p> required Source code in <code>matfree/funm.py</code> <pre><code>def funm_lanczos_sym(dense_funm: Callable, tridiag_sym: Callable, /) -&gt; Callable:\n    \"\"\"Implement a matrix-function-vector product via Lanczos' tridiagonalisation.\n\n    This algorithm uses Lanczos' tridiagonalisation\n    and therefore applies only to symmetric matrices.\n\n    Parameters\n    ----------\n    dense_funm\n        An implementation of a function of a dense matrix.\n        For example, the output of\n        [funm.dense_funm_sym_eigh][matfree.funm.dense_funm_sym_eigh]\n        [funm.dense_funm_schur][matfree.funm.dense_funm_schur]\n    tridiag_sym\n        An implementation of tridiagonalisation.\n        E.g., the output of\n        [decomp.tridiag_sym][matfree.decomp.tridiag_sym].\n    \"\"\"\n\n    def estimate(matvec: Callable, vec, *parameters):\n        length = linalg.vector_norm(vec)\n        vec /= length\n        Q, matrix, *_ = tridiag_sym(matvec, vec, *parameters)\n\n        funm = dense_funm(matrix)\n        e1 = np.eye(len(matrix))[0, :]\n        return length * (Q @ funm @ e1)\n\n    return estimate\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.integrand_funm_product","title":"<code>matfree.funm.integrand_funm_product(dense_funm, algorithm)</code>","text":"<p>Construct the integrand for matrix-function-trace estimation.</p> <p>Instead of the trace of a function of a matrix, compute the trace of a function of the product of matrices. Here, \"product\" refers to \\(X = A^\\top A\\).</p> Source code in <code>matfree/funm.py</code> <pre><code>def integrand_funm_product(dense_funm, algorithm, /):\n    r\"\"\"Construct the integrand for matrix-function-trace estimation.\n\n    Instead of the trace of a function of a matrix,\n    compute the trace of a function of the product of matrices.\n    Here, \"product\" refers to $X = A^\\top A$.\n    \"\"\"\n\n    def quadform(matvec, v0, *parameters):\n        v0_flat, v_unflatten = tree.ravel_pytree(v0)\n        length = linalg.vector_norm(v0_flat)\n        v0_flat /= length\n\n        def matvec_flat(v_flat, *p):\n            v = v_unflatten(v_flat)\n            Av = matvec(v, *p)\n            flat, _unflatten = tree.ravel_pytree(Av)\n            return flat\n\n        # Decompose into orthogonal-bidiag-orthogonal\n        _, B, *_ = algorithm(matvec_flat, v0_flat, *parameters)\n\n        # Evaluate matfun\n        fA = dense_funm(B)\n        e1 = np.eye(len(fA))[0, :]\n        return length**2 * linalg.inner(e1, fA @ e1)\n\n    return quadform\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.integrand_funm_product_logdet","title":"<code>matfree.funm.integrand_funm_product_logdet(bidiag: Callable)</code>","text":"<p>Construct the integrand for the log-determinant of a matrix-product.</p> <p>Here, \"product\" refers to \\(X = A^\\top A\\).</p> Source code in <code>matfree/funm.py</code> <pre><code>def integrand_funm_product_logdet(bidiag: Callable, /):\n    r\"\"\"Construct the integrand for the log-determinant of a matrix-product.\n\n    Here, \"product\" refers to $X = A^\\top A$.\n    \"\"\"\n    dense_funm = dense_funm_product_svd(np.log)\n    return integrand_funm_product(dense_funm, bidiag)\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.integrand_funm_product_schatten_norm","title":"<code>matfree.funm.integrand_funm_product_schatten_norm(power, bidiag: Callable)</code>","text":"<p>Construct the integrand for the \\(p\\)-th power of the Schatten-p norm.</p> Source code in <code>matfree/funm.py</code> <pre><code>def integrand_funm_product_schatten_norm(power, bidiag: Callable, /):\n    r\"\"\"Construct the integrand for the $p$-th power of the Schatten-p norm.\"\"\"\n\n    def matfun(x):\n        \"\"\"Matrix-function for Schatten-p norms.\"\"\"\n        return x ** (power / 2)\n\n    dense_funm = dense_funm_product_svd(matfun)\n    return integrand_funm_product(dense_funm, bidiag)\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.integrand_funm_sym","title":"<code>matfree.funm.integrand_funm_sym(dense_funm, tridiag_sym)</code>","text":"<p>Construct the integrand for matrix-function-trace estimation.</p> <p>This function assumes a symmetric matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dense_funm</code> <p>An implementation of a function of a dense matrix. For example, the output of funm.dense_funm_sym_eigh funm.dense_funm_schur</p> required <code>tridiag_sym</code> <p>An implementation of tridiagonalisation. E.g., the output of decomp.tridiag_sym.</p> required Source code in <code>matfree/funm.py</code> <pre><code>def integrand_funm_sym(dense_funm, tridiag_sym, /):\n    \"\"\"Construct the integrand for matrix-function-trace estimation.\n\n    This function assumes a symmetric matrix.\n\n    Parameters\n    ----------\n    dense_funm\n        An implementation of a function of a dense matrix.\n        For example, the output of\n        [funm.dense_funm_sym_eigh][matfree.funm.dense_funm_sym_eigh]\n        [funm.dense_funm_schur][matfree.funm.dense_funm_schur]\n    tridiag_sym\n        An implementation of tridiagonalisation.\n        E.g., the output of\n        [decomp.tridiag_sym][matfree.decomp.tridiag_sym].\n\n    \"\"\"\n\n    def quadform(matvec, v0, *parameters):\n        v0_flat, v_unflatten = tree.ravel_pytree(v0)\n        length = linalg.vector_norm(v0_flat)\n        v0_flat /= length\n\n        def matvec_flat(v_flat, *p):\n            v = v_unflatten(v_flat)\n            Av = matvec(v, *p)\n            flat, _unflatten = tree.ravel_pytree(Av)\n            return flat\n\n        _, dense, *_ = tridiag_sym(matvec_flat, v0_flat, *parameters)\n\n        fA = dense_funm(dense)\n        e1 = np.eye(len(fA))[0, :]\n        return length**2 * linalg.inner(e1, fA @ e1)\n\n    return quadform\n</code></pre>"},{"location":"API_documentation/funm/#matfree.funm.integrand_funm_sym_logdet","title":"<code>matfree.funm.integrand_funm_sym_logdet(tridiag_sym: Callable)</code>","text":"<p>Construct the integrand for the log-determinant.</p> <p>This function assumes a symmetric, positive definite matrix.</p> <p>Parameters:</p> Name Type Description Default <code>tridiag_sym</code> <code>Callable</code> <p>An implementation of tridiagonalisation. E.g., the output of decomp.tridiag_sym.</p> required Source code in <code>matfree/funm.py</code> <pre><code>def integrand_funm_sym_logdet(tridiag_sym: Callable, /):\n    \"\"\"Construct the integrand for the log-determinant.\n\n    This function assumes a symmetric, positive definite matrix.\n\n    Parameters\n    ----------\n    tridiag_sym\n        An implementation of tridiagonalisation.\n        E.g., the output of\n        [decomp.tridiag_sym][matfree.decomp.tridiag_sym].\n\n    \"\"\"\n    dense_funm = dense_funm_sym_eigh(np.log)\n    return integrand_funm_sym(dense_funm, tridiag_sym)\n</code></pre>"},{"location":"API_documentation/low_rank/","title":"matfree.low_rank","text":""},{"location":"API_documentation/low_rank/#matfree.low_rank","title":"<code>matfree.low_rank</code>","text":"<p>Low-rank approximations (like partial Cholesky decompositions) of matrices.</p>"},{"location":"API_documentation/low_rank/#matfree.low_rank.cholesky_partial","title":"<code>matfree.low_rank.cholesky_partial(mat_el: Callable, /, *, nrows: int, rank: int) -&gt; Callable</code>","text":"<p>Compute a partial Cholesky factorisation.</p> Source code in <code>matfree/low_rank.py</code> <pre><code>def cholesky_partial(mat_el: Callable, /, *, nrows: int, rank: int) -&gt; Callable:\n    \"\"\"Compute a partial Cholesky factorisation.\"\"\"\n\n    def cholesky(*params):\n        if rank &gt; nrows:\n            msg = f\"Rank exceeds n: {rank} &gt;= {nrows}.\"\n            raise ValueError(msg)\n        if rank &lt; 1:\n            msg = f\"Rank must be positive, but {rank} &lt; {1}.\"\n            raise ValueError(msg)\n\n        step = _cholesky_partial_body(mat_el, nrows, *params)\n        chol = np.zeros((nrows, rank))\n        return control_flow.fori_loop(0, rank, step, chol), {}\n\n    return cholesky\n</code></pre>"},{"location":"API_documentation/low_rank/#matfree.low_rank.cholesky_partial_pivot","title":"<code>matfree.low_rank.cholesky_partial_pivot(mat_el: Callable, /, *, nrows: int, rank: int) -&gt; Callable</code>","text":"<p>Compute a partial Cholesky factorisation with pivoting.</p> Source code in <code>matfree/low_rank.py</code> <pre><code>def cholesky_partial_pivot(mat_el: Callable, /, *, nrows: int, rank: int) -&gt; Callable:\n    \"\"\"Compute a partial Cholesky factorisation with pivoting.\"\"\"\n\n    def cholesky(*params):\n        if rank &gt; nrows:\n            msg = f\"Rank exceeds nrows: {rank} &gt;= {nrows}.\"\n            raise ValueError(msg)\n        if rank &lt; 1:\n            msg = f\"Rank must be positive, but {rank} &lt; {1}.\"\n            raise ValueError(msg)\n\n        body = _cholesky_partial_pivot_body(mat_el, nrows, *params)\n\n        L = np.zeros((nrows, rank))\n        P = np.arange(nrows)\n\n        init = (L, P, P, True)\n        (L, P, _matrix, success) = control_flow.fori_loop(0, rank, body, init)\n        return _pivot_invert(L, P), {\"success\": success}\n\n    return cholesky\n</code></pre>"},{"location":"API_documentation/low_rank/#matfree.low_rank.preconditioner","title":"<code>matfree.low_rank.preconditioner(cholesky: Callable) -&gt; Callable</code>","text":"<p>Turn a low-rank approximation into a preconditioner.</p> <p>Parameters:</p> Name Type Description Default <code>cholesky</code> <code>Callable</code> <p>(Partial) Cholesky decomposition. Usually, the result of either cholesky_partial or cholesky_partial_pivot.</p> required <p>Returns:</p> Type Description <code>solve</code> <p>A function that computes</p> \\[ (v, s, *p) \\mapsto (sI + L(*p) L(*p)^\\top)^{-1} v, \\] <p>where \\(K = [k(i,j,*p)]_{ij} \\approx L(*p) L(*p)^\\top\\) and \\(L\\) comes from the low-rank approximation.</p> Source code in <code>matfree/low_rank.py</code> <pre><code>def preconditioner(cholesky: Callable, /) -&gt; Callable:\n    r\"\"\"Turn a low-rank approximation into a preconditioner.\n\n    Parameters\n    ----------\n    cholesky\n        (Partial) Cholesky decomposition.\n        Usually, the result of either\n        [cholesky_partial][matfree.low_rank.cholesky_partial]\n        or\n        [cholesky_partial_pivot][matfree.low_rank.cholesky_partial_pivot].\n\n\n    Returns\n    -------\n    solve\n        A function that computes\n\n        $$\n        (v, s, *p) \\mapsto (sI + L(*p) L(*p)^\\top)^{-1} v,\n        $$\n\n        where $K = [k(i,j,*p)]_{ij} \\approx L(*p) L(*p)^\\top$\n        and $L$ comes from the low-rank approximation.\n    \"\"\"\n\n    def solve(v: Array, s: float, *cholesky_params):\n        chol, info = cholesky(*cholesky_params)\n\n        # Assert that the low-rank matrix is tall,\n        # not wide (every sign has a story...)\n        N, n = np.shape(chol)\n        assert n &lt;= N, (N, n)\n\n        # Scale\n        U = chol / np.sqrt(s)\n        V = chol.T / np.sqrt(s)\n        v /= s\n\n        # Cholesky decompose the capacitance matrix\n        # and solve the system\n        eye_n = np.eye(n)\n        chol_cap = linalg.cho_factor(eye_n + V @ U)\n        sol = linalg.cho_solve(chol_cap, V @ v)\n        return v - U @ sol, info\n\n    return solve\n</code></pre>"},{"location":"API_documentation/lstsq/","title":"matfree.lstsq","text":""},{"location":"API_documentation/lstsq/#matfree.lstsq","title":"<code>matfree.lstsq</code>","text":"<p>Matrix-free algorithms for least-squares-type problems.</p>"},{"location":"API_documentation/lstsq/#matfree.lstsq.lsmr","title":"<code>matfree.lstsq.lsmr(*, atol: float = 1e-06, btol: float = 1e-06, ctol: float = 1e-08, maxiter: int = 1000000, while_loop: Callable = control_flow.while_loop, custom_vjp: bool = True, is_full_rank: bool = False)</code>","text":"<p>Construct an experimental implementation of LSMR.</p> <p>Follows the SciPy's implementation, but uses JAX. LSMR is due to Fong and Saunders (2011):</p> BibTex for Fong and Saunders (2011) <pre><code>@article{fong2011lsmr,\n    title={{LSMR}: An iterative algorithm for sparse least-squares problems},\n    author={Fong, David Chin-Lung and Saunders, Michael},\n    journal={SIAM Journal on Scientific Computing},\n    volume={33},\n    number={5},\n    pages={2950--2971},\n    year={2011},\n    publisher={SIAM}\n}\n</code></pre> <p>Setting <code>custom_vjp</code> to <code>True</code> implies using the low-memory gradients of matrix-free least squares, according to what has been proposed by Roy et al. (2025). Here is a link to the preprint. These gradients are exact, so there is little reason not to use them. If you use this configuration, please cite Roy et al. (2025):</p> BibTex for Roy et al. (2025) <pre><code>@article{roy2025matrix,\n    title={Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them},\n    author={Roy, Hrittik and Hauberg, S{\\\\o}ren and Kr{\\\"a}mer, Nicholas},\n    journal={arXiv preprint arXiv:2510.19634},\n    year={2025}\n}\n</code></pre> Source code in <code>matfree/lstsq.py</code> <pre><code>def lsmr(\n    *,\n    atol: float = 1e-6,\n    btol: float = 1e-6,\n    ctol: float = 1e-8,\n    maxiter: int = 1_000_000,\n    while_loop: Callable = control_flow.while_loop,\n    custom_vjp: bool = True,\n    # if the matrix is full rank, the backward pass can be accelerated:\n    is_full_rank: bool = False,\n):\n    r\"\"\"Construct an experimental implementation of LSMR.\n\n    Follows the [SciPy's implementation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html),\n    but uses JAX.\n    LSMR is due to Fong and Saunders (2011):\n\n    ??? note \"BibTex for Fong and Saunders (2011)\"\n        ```bibtex\n        @article{fong2011lsmr,\n            title={{LSMR}: An iterative algorithm for sparse least-squares problems},\n            author={Fong, David Chin-Lung and Saunders, Michael},\n            journal={SIAM Journal on Scientific Computing},\n            volume={33},\n            number={5},\n            pages={2950--2971},\n            year={2011},\n            publisher={SIAM}\n        }\n        ```\n\n    Setting `custom_vjp` to `True` implies using the low-memory\n    gradients of matrix-free least squares,\n    according to what has been proposed by Roy et al. (2025).\n    [Here](https://arxiv.org/abs/2510.19634) is a link to the preprint.\n    These gradients are exact, so there is little reason not to use them.\n    If you use this configuration, please cite Roy et al. (2025):\n\n    ??? note \"BibTex for Roy et al. (2025)\"\n        ```bibtex\n        @article{roy2025matrix,\n            title={Matrix-Free Least Squares Solvers: Values, Gradients, and What to Do With Them},\n            author={Roy, Hrittik and Hauberg, S{\\\\o}ren and Kr{\\\"a}mer, Nicholas},\n            journal={arXiv preprint arXiv:2510.19634},\n            year={2025}\n        }\n        ```\n    \"\"\"\n    # todo: stop iteration when NaN or Inf are detected.\n\n    @tree.register_dataclass\n    @containers.dataclass\n    class State:\n        \"\"\"LSMR state.\"\"\"\n\n        # Iteration count:\n        itn: int\n        # Bidiagonalisation variables:\n        alpha: float\n        u: Array\n        v: Array\n        # LSMR-specific variables:\n        alphabar: float\n        rhobar: float\n        rho: float\n        zeta: float\n        sbar: float\n        cbar: float\n        zetabar: float\n        hbar: Array\n        h: Array\n        x: Array\n        # Variables for estimation of ||r||:\n        betadd: float\n        thetatilde: float\n        rhodold: float\n        betad: float\n        tautildeold: float\n        d: float\n        # Variables for estimation of ||A|| and cond(A)\n        normA2: float\n        maxrbar: float\n        minrbar: float\n        normA: float\n        condA: float\n        normx: float\n\n        # Variables for use in stopping rules\n        normar: float\n        normr: float\n        # Reason for stopping\n        istop: int\n\n    # more often than not, the matvec is defined after the LSMR\n    # solver has been constructed. So it's part of the run()\n    # function, not the LSMR constructor.\n    def run(vecmat, b, *vecmat_args, x0=None, damp=0.0):\n        \"\"\"Run LSMR.\n\n        Matches Scipy's implementation.\n\n        Warning: gradients wrt \"x0\" and \"damp\" are not defined.\n        \"\"\"\n        x_like = func.eval_shape(vecmat, b, *vecmat_args)\n        (ncols,) = x_like.shape\n        x = x0 if x0 is not None else np.zeros(ncols, dtype=b.dtype)\n\n        # Combine the lstsq_fun with a closure convert, because\n        # typically, vecmat is a lambda function and if we want to\n        # have explicit parameter-VJPs, all parameters need to be explicit.\n        # This means that in this function here, we always use lstsq_public\n        # (and return lstsq_public!), but provide lstsq_fun with the custom VJP.\n        # Thereby, the function that gets the custom VJP is, from now on, only\n        # called after a previous call to closure convert which 'fixes' all namespaces.\n        def vecmat_noargs(v):\n            return vecmat(v, *vecmat_args)\n\n        vecmat_closure, args = func.closure_convert(vecmat_noargs, b)\n        return _run(vecmat_closure, b, args, x, damp)\n\n    def _run(vecmat, b, vecmat_args, x0, damp):\n        def vecmat_noargs(v):\n            return vecmat(v, *vecmat_args)\n\n        def matvec_noargs(w):\n            matvec = func.linear_transpose(vecmat_noargs, b)\n            (Aw,) = matvec(w)\n            return Aw\n\n        state, normb = init(matvec_noargs, vecmat_noargs, b, x0)\n        step_fun = make_step(matvec_noargs, vecmat_noargs, normb=normb, damp=damp)\n        cond_fun = make_cond_fun()\n        state = while_loop(cond_fun, step_fun, state)\n\n        stats_ = stats(state)\n        return state.x, stats_\n\n    def init(matvec_noargs, vecmat_noargs, b, x):\n        normb = linalg.vector_norm(b)\n\n        Ax = matvec_noargs(x)\n        u = b - Ax\n        beta = linalg.vector_norm(u)\n        u = u / np.where(beta &gt; 0, beta, 1.0)\n\n        v = vecmat_noargs(u)\n        alpha = linalg.vector_norm(v)\n        v = v / np.where(alpha &gt; 0, alpha, 1)\n        v = np.where(beta == 0, np.zeros_like(v), v)\n        alpha = np.where(beta == 0, np.zeros_like(alpha), alpha)\n\n        # Initialize variables for 1st iteration.\n\n        zetabar = alpha * beta\n        alphabar = alpha\n        rho = 1.0\n        rhobar = 1.0\n        cbar = 1.0\n        sbar = 0.0\n\n        h = v\n        hbar = np.zeros_like(x)\n\n        # Initialize variables for estimation of ||r||.\n\n        betadd = beta\n        betad = 0.0\n        rhodold = 1.0\n        tautildeold = 0.0\n        thetatilde = 0.0\n        zeta = 0.0\n        d = 0.0\n\n        # Initialize variables for estimation of ||A|| and cond(A)\n\n        normA2 = alpha * alpha\n        maxrbar = 0.0\n        minrbar = 1e10\n        normA = np.sqrt(normA2)\n        condA = 1.0\n        normx = 0.0\n\n        # Items for use in stopping rules, normb set earlier\n        normr = beta\n\n        # Reverse the order here from the original matlab code because\n        # there was an error on return when arnorm==0\n        normar = alpha * beta\n\n        # Main iteration loop.\n        state = State(  # type: ignore\n            itn=0,\n            alpha=alpha,\n            u=u,\n            v=v,\n            alphabar=alphabar,\n            rho=rho,\n            rhobar=rhobar,\n            zeta=zeta,\n            sbar=sbar,\n            cbar=cbar,\n            zetabar=zetabar,\n            hbar=hbar,\n            h=h,\n            x=x,\n            betadd=betadd,\n            thetatilde=thetatilde,\n            rhodold=rhodold,\n            betad=betad,\n            tautildeold=tautildeold,\n            d=d,\n            normA2=normA2,\n            maxrbar=maxrbar,\n            minrbar=minrbar,\n            normar=normar,\n            normr=normr,\n            normA=normA,\n            condA=condA,\n            normx=normx,\n            istop=0,\n        )\n        state = tree.tree_map(np.asarray, state)\n        return state, normb\n\n    def make_step(matvec, vecmat, normb: float, damp: float) -&gt; Callable:\n        def step(state: State) -&gt; State:\n            # Perform the next step of the bidiagonalization\n\n            Av = matvec(state.v)\n            u = Av - state.alpha * state.u\n            beta = linalg.vector_norm(u)\n\n            u = u / np.where(beta &gt; 0, beta, 1.0)\n            v = vecmat(u) - beta * state.v\n            alpha = linalg.vector_norm(v)\n            v = v / np.where(alpha &gt; 0, alpha, 1)\n\n            # Construct rotation Qhat_{k,2k+1}.\n\n            chat, shat, alphahat = _sym_ortho(state.alphabar, damp)\n\n            # Use a plane rotation (Q_i) to turn B_i to R_i\n\n            rhoold = state.rho\n            c, s, rho = _sym_ortho(alphahat, beta)\n            thetanew = s * alpha\n            alphabar = c * alpha\n\n            # Use a plane rotation (Qbar_i) to turn R_i^T to R_i^bar\n\n            rhobarold = state.rhobar\n            zetaold = state.zeta\n            thetabar = state.sbar * rho\n            rhotemp = state.cbar * rho\n            cbar, sbar, rhobar = _sym_ortho(rhotemp, thetanew)\n            zeta = cbar * state.zetabar\n            zetabar = -sbar * state.zetabar\n\n            # Update h, h_hat, x.\n\n            hbar = state.h - state.hbar * (thetabar * rho / (rhoold * rhobarold))\n            x = state.x + (zeta / (rho * rhobar)) * hbar\n            h = v - state.h * (thetanew / rho)\n\n            # Estimate of ||r||.\n\n            # Apply rotation Qhat_{k,2k+1}.\n            betaacute = chat * state.betadd\n            betacheck = -shat * state.betadd\n\n            # Apply rotation Q_{k,k+1}.\n            betahat = c * betaacute\n            betadd = -s * betaacute\n\n            # Apply rotation Qtilde_{k-1}.\n\n            thetatildeold = state.thetatilde\n            ctildeold, stildeold, rhotildeold = _sym_ortho(state.rhodold, thetabar)\n            thetatilde = stildeold * rhobar\n            rhodold = ctildeold * rhobar\n            betad = -stildeold * state.betad + ctildeold * betahat\n\n            tautildeold = (zetaold - thetatildeold * state.tautildeold) / rhotildeold\n            taud = (zeta - thetatilde * tautildeold) / rhodold\n            d = state.d + betacheck * betacheck\n            normr = np.sqrt(d + (betad - taud) ** 2 + betadd * betadd)\n\n            # Estimate ||A||.\n            normA2 = state.normA2 + beta * beta\n            normA = np.sqrt(normA2)\n            normA2 = normA2 + alpha * alpha\n\n            # Estimate cond(A).\n            maxrbar = np.elementwise_max(state.maxrbar, rhobarold)\n            minrbar = np.where(\n                state.itn &gt; 1,\n                np.elementwise_min(state.minrbar, rhobarold),\n                state.minrbar,\n            )\n            condA = np.elementwise_max(maxrbar, rhotemp) / np.elementwise_min(\n                minrbar, rhotemp\n            )\n\n            # Compute norms for convergence testing.\n            normar = np.abs(zetabar)\n            normx = linalg.vector_norm(x)\n\n            # Check whether we should stop\n            itn = state.itn + 1\n            test1 = normr / normb\n            z = normA * normr\n            z_safe = np.where(z != 0, z, 1.0)\n            test2 = np.where(z != 0, normar / z_safe, _LARGE_VALUE)\n            test3 = 1 / condA\n            t1 = test1 / (1 + normA * normx / normb)\n            rtol = btol + atol * normA * normx / normb\n\n            # Early exits\n            istop = 0\n            istop = np.where(normar == 0, 9, istop)\n            istop = np.where(normb == 0, 8, istop)\n            istop = np.where(itn &gt;= maxiter, 7, istop)\n            istop = np.where(1 + test3 &lt;= 1, 6, istop)\n            istop = np.where(1 + test2 &lt;= 1, 5, istop)\n            istop = np.where(1 + t1 &lt;= 1, 4, istop)\n            istop = np.where(test3 &lt;= ctol, 3, istop)\n            istop = np.where(test2 &lt;= atol, 2, istop)\n            istop = np.where(test1 &lt;= rtol, 1, istop)\n\n            return State(  # type: ignore\n                itn=itn,\n                alpha=alpha,\n                u=u,\n                v=v,\n                alphabar=alphabar,\n                rho=rho,\n                rhobar=rhobar,\n                zeta=zeta,\n                sbar=sbar,\n                cbar=cbar,\n                zetabar=zetabar,\n                hbar=hbar,\n                h=h,\n                x=x,\n                betadd=betadd,\n                thetatilde=thetatilde,\n                rhodold=rhodold,\n                betad=betad,\n                tautildeold=tautildeold,\n                d=d,\n                normA2=normA2,\n                maxrbar=maxrbar,\n                minrbar=minrbar,\n                normar=normar,\n                normr=normr,\n                normA=normA,\n                condA=condA,\n                normx=normx,\n                istop=istop,\n            )\n\n        return step\n\n    def make_cond_fun() -&gt; Callable:\n        def cond(state):\n            state_flat, _ = tree.ravel_pytree(state)\n            no_nans = np.logical_not(np.any(np.isnan(state_flat)))\n            proceed = np.where(state.istop == 0, True, False)\n            return np.logical_and(proceed, no_nans)\n\n        return cond\n\n    def stats(state: State) -&gt; dict:\n        return {\n            \"iteration_count\": state.itn,\n            \"norm_residual\": state.normr,\n            \"norm_At_residual\": state.normar,\n            \"norm_A\": state.normA,\n            \"cond_A\": state.condA,\n            \"norm_x\": state.normx,\n            \"istop\": state.istop,\n        }\n\n    if custom_vjp:\n        _run = _lstsq_custom_vjp(_run, is_full_rank=is_full_rank)\n    return run\n</code></pre>"},{"location":"API_documentation/stochtrace/","title":"matfree.stochtrace","text":""},{"location":"API_documentation/stochtrace/#matfree.stochtrace","title":"<code>matfree.stochtrace</code>","text":"<p>Stochastic estimation of traces, diagonals, and more.</p>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.estimator","title":"<code>matfree.stochtrace.estimator(integrand: Callable, /, sampler: Callable) -&gt; Callable</code>","text":"<p>Construct a stochastic trace-/diagonal-estimator.</p> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <code>Callable</code> <p>The integrand function. For example, the return-value of integrand_trace. But any other integrand works, too.</p> required <code>sampler</code> <code>Callable</code> <p>The sample function. Usually, either sampler_normal or sampler_rademacher.</p> required <p>Returns:</p> Type Description <code>estimate</code> <p>A function that maps a random key to an estimate. This function can be compiled, vectorised, differentiated, or looped over as the user desires.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def estimator(integrand: Callable, /, sampler: Callable) -&gt; Callable:\n    \"\"\"Construct a stochastic trace-/diagonal-estimator.\n\n    Parameters\n    ----------\n    integrand\n        The integrand function. For example, the return-value of\n        [integrand_trace][matfree.stochtrace.integrand_trace].\n        But any other integrand works, too.\n    sampler\n        The sample function. Usually, either\n        [sampler_normal][matfree.stochtrace.sampler_normal] or\n        [sampler_rademacher][matfree.stochtrace.sampler_rademacher].\n\n    Returns\n    -------\n    estimate\n        A function that maps a random key to an estimate.\n        This function can be compiled, vectorised, differentiated,\n        or looped over as the user desires.\n\n    \"\"\"\n\n    def estimate(matvecs, key, *parameters):\n        samples = sampler(key)\n        Qs = func.vmap(lambda vec: integrand(matvecs, vec, *parameters))(samples)\n        return tree.tree_map(lambda s: np.mean(s, axis=0), Qs)\n\n    return estimate\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.integrand_diagonal","title":"<code>matfree.stochtrace.integrand_diagonal()</code>","text":"<p>Construct the integrand for estimating the diagonal.</p> <p>When plugged into the Monte-Carlo estimator, the result will be an Array or PyTree of Arrays with the same tree-structure as <code>matvec(*args_like)</code> where <code>*args_like</code> is an argument of the sampler.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def integrand_diagonal():\n    \"\"\"Construct the integrand for estimating the diagonal.\n\n    When plugged into the Monte-Carlo estimator,\n    the result will be an Array or PyTree of Arrays with the\n    same tree-structure as\n    ``\n    matvec(*args_like)\n    ``\n    where ``*args_like`` is an argument of the sampler.\n    \"\"\"\n\n    def integrand(matvec, v, *parameters):\n        Qv = matvec(v, *parameters)\n        v_flat, unflatten = tree.ravel_pytree(v)\n        Qv_flat, _unflatten = tree.ravel_pytree(Qv)\n        return unflatten(v_flat * Qv_flat)\n\n    return integrand\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.integrand_frobeniusnorm_squared","title":"<code>matfree.stochtrace.integrand_frobeniusnorm_squared()</code>","text":"<p>Construct the integrand for estimating the squared Frobenius norm.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def integrand_frobeniusnorm_squared():\n    \"\"\"Construct the integrand for estimating the squared Frobenius norm.\"\"\"\n\n    def integrand(matvec, vec, *parameters):\n        x = matvec(vec, *parameters)\n        v_flat, _unflatten = tree.ravel_pytree(x)\n        return linalg.inner(v_flat, v_flat)\n\n    return integrand\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.integrand_trace","title":"<code>matfree.stochtrace.integrand_trace()</code>","text":"<p>Construct the integrand for estimating the trace.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def integrand_trace():\n    \"\"\"Construct the integrand for estimating the trace.\"\"\"\n\n    def integrand(matvec, v, *parameters):\n        Qv = matvec(v, *parameters)\n        v_flat, _unflatten = tree.ravel_pytree(v)\n        Qv_flat, _unflatten = tree.ravel_pytree(Qv)\n        return linalg.inner(v_flat, Qv_flat)\n\n    return integrand\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.integrand_trace_and_diagonal","title":"<code>matfree.stochtrace.integrand_trace_and_diagonal()</code>","text":"<p>Construct the integrand for estimating the trace and diagonal jointly.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def integrand_trace_and_diagonal():\n    \"\"\"Construct the integrand for estimating the trace and diagonal jointly.\"\"\"\n\n    def integrand(matvec, v, *parameters):\n        Qv = matvec(v, *parameters)\n        v_flat, unflatten = tree.ravel_pytree(v)\n        Qv_flat, _unflatten = tree.ravel_pytree(Qv)\n        trace_form = linalg.inner(v_flat, Qv_flat)\n        diagonal_form = unflatten(v_flat * Qv_flat)\n        return {\"trace\": trace_form, \"diagonal\": diagonal_form}\n\n    return integrand\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.integrand_wrap_moments","title":"<code>matfree.stochtrace.integrand_wrap_moments(integrand, /, moments)</code>","text":"<p>Wrap an integrand into another integrand that computes moments.</p> <p>Parameters:</p> Name Type Description Default <code>integrand</code> <p>Any integrand function compatible with Hutchinson-style estimation.</p> required <code>moments</code> <p>Any Pytree (tuples, lists, dictionaries) whose leafs that are valid inputs to <code>lambda m: x**m</code> for an array <code>x</code>, usually, with data-type <code>float</code> (but that depends on the wrapped integrand). For example, <code>moments=4</code>, <code>moments=(1,2)</code>, or <code>moments={\"a\": 1, \"b\": 2}</code>.</p> required <p>Returns:</p> Type Description <code>integrand</code> <p>An integrand function compatible with Hutchinson-style estimation whose output has a PyTree-structure that mirrors the structure of the <code>moments</code> argument.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def integrand_wrap_moments(integrand, /, moments):\n    \"\"\"Wrap an integrand into another integrand that computes moments.\n\n    Parameters\n    ----------\n    integrand\n        Any integrand function compatible with Hutchinson-style estimation.\n    moments\n        Any Pytree (tuples, lists, dictionaries) whose leafs that are\n        valid inputs to ``lambda m: x**m`` for an array ``x``,\n        usually, with data-type ``float``\n        (but that depends on the wrapped integrand).\n        For example, ``moments=4``, ``moments=(1,2)``,\n        or ``moments={\"a\": 1, \"b\": 2}``.\n\n    Returns\n    -------\n    integrand\n        An integrand function compatible with Hutchinson-style estimation whose\n        output has a PyTree-structure that mirrors the structure of the ``moments``\n        argument.\n\n    \"\"\"\n\n    def integrand_wrapped(vec, *parameters):\n        Qs = integrand(vec, *parameters)\n        return tree.tree_map(moment_fun, Qs)\n\n    def moment_fun(x, /):\n        return tree.tree_map(lambda m: x**m, moments)\n\n    return integrand_wrapped\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.sampler_normal","title":"<code>matfree.stochtrace.sampler_normal(*args_like, num)</code>","text":"<p>Construct a function that samples from a standard-normal distribution.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def sampler_normal(*args_like, num):\n    \"\"\"Construct a function that samples from a standard-normal distribution.\"\"\"\n    return _sampler_from_jax_random(prng.normal, *args_like, num=num)\n</code></pre>"},{"location":"API_documentation/stochtrace/#matfree.stochtrace.sampler_rademacher","title":"<code>matfree.stochtrace.sampler_rademacher(*args_like, num)</code>","text":"<p>Construct a function that samples from a Rademacher distribution.</p> Source code in <code>matfree/stochtrace.py</code> <pre><code>def sampler_rademacher(*args_like, num):\n    \"\"\"Construct a function that samples from a Rademacher distribution.\"\"\"\n    return _sampler_from_jax_random(prng.rademacher, *args_like, num=num)\n</code></pre>"},{"location":"API_documentation/test_util/","title":"matfree.test_util","text":""},{"location":"API_documentation/test_util/#matfree.test_util","title":"<code>matfree.test_util</code>","text":"<p>Test utilities.</p>"},{"location":"API_documentation/test_util/#matfree.test_util.assert_allclose","title":"<code>matfree.test_util.assert_allclose(a, b)</code>","text":"<p>Assert that two arrays are close.</p> <p>This function uses a different default tolerance to jax.numpy.allclose. Instead of fixing values, the tolerance depends on the floating-point precision of the input variables.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def assert_allclose(a, b, /):\n    \"\"\"Assert that two arrays are close.\n\n    This function uses a different default tolerance to\n    jax.numpy.allclose. Instead of fixing values, the tolerance\n    depends on the floating-point precision of the input variables.\n    \"\"\"\n    a = np.asarray(a)\n    b = np.asarray(b)\n    tol = 10 * np.sqrt(np.finfo_eps(np.dtype(a)))\n\n    # For double precision sqrt(eps) is very tight...\n    if tol &lt; 1e-6:\n        tol *= 10\n    assert np.allclose(a, b, atol=tol, rtol=tol)\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.assert_columns_orthonormal","title":"<code>matfree.test_util.assert_columns_orthonormal(Q)</code>","text":"<p>Assert that the columns in a matrix are orthonormal.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def assert_columns_orthonormal(Q, /):\n    \"\"\"Assert that the columns in a matrix are orthonormal.\"\"\"\n    eye_like = Q.T @ Q\n    ref = np.eye(len(eye_like))\n    assert_allclose(eye_like, ref)\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.asymmetric_matrix_from_singular_values","title":"<code>matfree.test_util.asymmetric_matrix_from_singular_values(vals, /, nrows, ncols)</code>","text":"<p>Generate an asymmetric matrix with specific singular values.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def asymmetric_matrix_from_singular_values(vals, /, nrows, ncols):\n    \"\"\"Generate an asymmetric matrix with specific singular values.\"\"\"\n    A = np.reshape(np.arange(1.0, nrows * ncols + 1.0), (nrows, ncols))\n    A /= nrows * ncols\n    U, _S, Vt = linalg.svd(A, full_matrices=False)\n    return U @ linalg.diagonal(vals) @ Vt\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.symmetric_matrix_from_eigenvalues","title":"<code>matfree.test_util.symmetric_matrix_from_eigenvalues(eigvals)</code>","text":"<p>Generate a symmetric matrix with prescribed eigenvalues.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def symmetric_matrix_from_eigenvalues(eigvals, /):\n    \"\"\"Generate a symmetric matrix with prescribed eigenvalues.\"\"\"\n    (n,) = eigvals.shape\n\n    # Need _some_ matrix to start with\n    A = np.reshape(np.arange(1.0, n**2 + 1.0), (n, n))\n    A = A / linalg.matrix_norm(A, which=\"fro\")\n    X = A.T @ A + np.eye(n)\n\n    # QR decompose. We need the orthogonal matrix.\n    # Treat Q as a stack of eigenvectors.\n    Q, _R = linalg.qr_reduced(X)\n\n    # Treat Q as eigenvectors, and 'D' as eigenvalues.\n    # return Q D Q.T.\n    # This matrix will be dense, symmetric, and have a given spectrum.\n    return Q @ (eigvals[:, None] * Q.T)\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.to_dense_bidiag","title":"<code>matfree.test_util.to_dense_bidiag(d, e, /, offset=1)</code>","text":"<p>Materialize a bidiagonal matrix.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def to_dense_bidiag(d, e, /, offset=1):\n    \"\"\"Materialize a bidiagonal matrix.\"\"\"\n    diag = linalg.diagonal_matrix(d)\n    offdiag = linalg.diagonal_matrix(e, offset=offset)\n    return diag + offdiag\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.to_dense_tridiag_sym","title":"<code>matfree.test_util.to_dense_tridiag_sym(d, e)</code>","text":"<p>Materialize a symmetric tridiagonal matrix.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def to_dense_tridiag_sym(d, e, /):\n    \"\"\"Materialize a symmetric tridiagonal matrix.\"\"\"\n    diag = linalg.diagonal_matrix(d)\n    offdiag1 = linalg.diagonal_matrix(e, offset=1)\n    offdiag2 = linalg.diagonal_matrix(e, offset=-1)\n    return diag + offdiag1 + offdiag2\n</code></pre>"},{"location":"API_documentation/test_util/#matfree.test_util.tree_random_like","title":"<code>matfree.test_util.tree_random_like(key, pytree, *, generate_func=prng.normal)</code>","text":"<p>Fill a tree with random values.</p> Source code in <code>matfree/test_util.py</code> <pre><code>def tree_random_like(key, pytree, *, generate_func=prng.normal):\n    \"\"\"Fill a tree with random values.\"\"\"\n    flat, unflatten = tree.ravel_pytree(pytree)\n    flat_like = generate_func(key, shape=flat.shape, dtype=flat.dtype)\n    return unflatten(flat_like)\n</code></pre>"},{"location":"Developer_documentation/1_use_matfree%27s_continuous_integration/","title":"Use Matfree's continuous integration","text":"<p>To install all test-related dependencies (assuming JAX is installed; if not, run <code>pip install .[cpu]</code>), execute</p> <pre><code>pip install .[test]\n</code></pre> <p>Then, run the tests via</p> <pre><code>make test\n</code></pre> <p>Install all formatting-related dependencies via</p> <pre><code>pip install .[format-and-lint]\npre-commit install\n</code></pre> <p>and format the code via</p> <pre><code>make format-and-lint\n</code></pre> <p>Install the documentation-related dependencies as</p> <pre><code>pip install .[doc]\n</code></pre> <p>Preview the documentation via</p> <pre><code>make doc-preview\n</code></pre> <p>Check whether the docs can be built correctly via</p> <pre><code>make doc-build\n</code></pre>"},{"location":"Developer_documentation/2_contribute_to_matfree/","title":"Contribute to Matfree","text":"<p>Contributions are welcome!</p> <p>Issues:</p> <p>Most contributions start with an issue. Please don't hesitate to create issues in which you ask for features, give performance feedback, or simply want to reach out.</p> <p>Pull requests:</p> <p>To make a pull request, proceed as follows:</p> <ul> <li>Fork the repository.</li> <li>Install all dependencies with <code>pip install .[full]</code> or <code>pip install -e .[full]</code>.</li> <li>Make your changes.</li> <li>From the project's root, run the tests via <code>make test</code>. Check out <code>make format-and-lint</code> as well. Use the pre-commit hook if you like.</li> <li>Then, make the pull request. Choose an informative title (have a look at previous PRs if you need inspiration), link all related issues, describe the change, and pick a label (e.g. \"documentation\", or \"enhancement\"). The label is important because the release notes group the pull requests by label.</li> </ul> <p>When making a pull request, keep in mind the following (rough) guidelines:</p> <ul> <li>Most PRs resolve an issue.</li> <li>Most PRs contain a single commit. Here is how we can write better commit messages.</li> <li>Most enhancements (e.g. new features) are covered by tests.</li> </ul> <p>Here is what GitHub considers important for informative pull requests.</p>"},{"location":"Developer_documentation/3_extend_matfree%27s_documentation/","title":"Extend Matfree's documentation","text":"<p>Write a new tutorial:</p> <p>To add a new tutorial, create a Python file in <code>tutorials/</code> and fill it with content. Use docstrings (mirror the style in the existing tutorials). Make sure to satisfy the formatting- and linting-requirements. That's all.</p> <p>Then, the documentation pipeline will automatically convert those into a format compatible with Jupytext, which is subsequently included in the documentation. If you do not want to make the tutorial part of the documentation, make the filename have a leading underscore.</p> <p>Extend the developer documentation:</p> <p>To extend the developer documentation, create a new section in the README. Use a second-level header (which is a header starting with \"##\") and fill the section with content. Then, the documentation pipeline will turn this section into a page in the developer documentation.</p> <p>Create a new module:</p> <p>To make a new module appear in the documentation, create the new module in <code>matfree/</code>, and fill it with content. Unless the module name starts with an underscore or is placed in the backend, the documentation pipeline will take care of the rest.</p>"},{"location":"Developer_documentation/4_understand_matfree%27s_api_policy/","title":"Understand Matfree's API policy","text":"<p>Matfree is a research project, and parts of its API may change frequently and without warning.</p> <p>This stage of development aligns with its current (0.y.z) version. To quote from semantic versioning:</p> <p>Major version zero (0.y.z) is for initial development. Anything MAY change at any time. The public API SHOULD NOT be considered stable.</p> <p>Matfree does not implement an official deprecation policy (just yet) but handles all API change communication via version increments:</p> <ul> <li>If a change is backwards compatible (e.g., a backwards-compatible new feature or a bugfix), the patch version is incremented, e.g., from <code>v0.1.5</code> to <code>v0.1.6</code>.</li> <li>If a change is not backwards compatible, the minor version is incremented, e.g., from <code>v0.1.6</code> to <code>v0.2.0</code>.</li> </ul>"},{"location":"Tutorials/1_compute_log_determinants_with_stochastic_lanczos_quadrature/","title":"Compute log-determinants with stochastic Lanczos quadrature","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[2]: Copied! <pre>from matfree import decomp, funm, stochtrace\n</pre> from matfree import decomp, funm, stochtrace <p>Set up a matrix.</p> In\u00a0[3]: Copied! <pre>nhidden, nrows = 6, 5\nA = jnp.reshape(jnp.arange(1.0, 1.0 + nhidden * nrows), (nhidden, nrows))\nA /= nhidden * nrows\n</pre> nhidden, nrows = 6, 5 A = jnp.reshape(jnp.arange(1.0, 1.0 + nhidden * nrows), (nhidden, nrows)) A /= nhidden * nrows In\u00a0[4]: Copied! <pre>def matvec(x):\n    \"\"\"Compute a matrix-vector product.\"\"\"\n    return A.T @ (A @ x) + x\n</pre> def matvec(x):     \"\"\"Compute a matrix-vector product.\"\"\"     return A.T @ (A @ x) + x In\u00a0[5]: Copied! <pre>x_like = jnp.ones((nrows,), dtype=float)  # use to determine shapes of vectors\n</pre> x_like = jnp.ones((nrows,), dtype=float)  # use to determine shapes of vectors <p>Estimate log-determinants with stochastic Lanczos quadrature.</p> In\u00a0[6]: Copied! <pre>num_matvecs = 3\ntridiag_sym = decomp.tridiag_sym(num_matvecs)\nproblem = funm.integrand_funm_sym_logdet(tridiag_sym)\nsampler = stochtrace.sampler_normal(x_like, num=1_000)\nestimator = stochtrace.estimator(problem, sampler=sampler)\nlogdet = estimator(matvec, jax.random.PRNGKey(1))\nprint(logdet)\n</pre> num_matvecs = 3 tridiag_sym = decomp.tridiag_sym(num_matvecs) problem = funm.integrand_funm_sym_logdet(tridiag_sym) sampler = stochtrace.sampler_normal(x_like, num=1_000) estimator = stochtrace.estimator(problem, sampler=sampler) logdet = estimator(matvec, jax.random.PRNGKey(1)) print(logdet) <pre>2.5452628\n</pre> <p>For comparison:</p> In\u00a0[7]: Copied! <pre>print(jnp.linalg.slogdet(A.T @ A + jnp.eye(nrows)))\n</pre> print(jnp.linalg.slogdet(A.T @ A + jnp.eye(nrows))) <pre>SlogdetResult(sign=Array(1., dtype=float32), logabsdet=Array(2.456815, dtype=float32))\n</pre> <p>We can compute the log determinant of a matrix of the form $M = B^\\top B$, purely based on arithmetic with $B$; no need to assemble $M$:</p> In\u00a0[8]: Copied! <pre>A = jnp.reshape(jnp.arange(1.0, 1.0 + nrows**2), (nrows, nrows))\nA += jnp.eye(nrows)\nA /= nrows**2\n</pre> A = jnp.reshape(jnp.arange(1.0, 1.0 + nrows**2), (nrows, nrows)) A += jnp.eye(nrows) A /= nrows**2 In\u00a0[9]: Copied! <pre>def matvec_half(x):\n    \"\"\"Compute a matrix-vector product.\"\"\"\n    return A @ x\n</pre> def matvec_half(x):     \"\"\"Compute a matrix-vector product.\"\"\"     return A @ x In\u00a0[10]: Copied! <pre>num_matvecs = 3\nbidiag = decomp.bidiag(num_matvecs)\nproblem = funm.integrand_funm_product_logdet(bidiag)\nsampler = stochtrace.sampler_normal(x_like, num=1_000)\nestimator = stochtrace.estimator(problem, sampler=sampler)\nlogdet = estimator(matvec_half, jax.random.PRNGKey(1))\nprint(logdet)\n</pre> num_matvecs = 3 bidiag = decomp.bidiag(num_matvecs) problem = funm.integrand_funm_product_logdet(bidiag) sampler = stochtrace.sampler_normal(x_like, num=1_000) estimator = stochtrace.estimator(problem, sampler=sampler) logdet = estimator(matvec_half, jax.random.PRNGKey(1)) print(logdet) <pre>-21.461075\n</pre> <p>Internally, Matfree uses JAX's vector-Jacobian products to turn the matrix-vector product into a vector-matrix product.</p> <p>For comparison:</p> In\u00a0[11]: Copied! <pre>print(jnp.linalg.slogdet(A.T @ A))\n</pre> print(jnp.linalg.slogdet(A.T @ A)) <pre>SlogdetResult(sign=Array(1., dtype=float32), logabsdet=Array(-21.758816, dtype=float32))\n</pre>"},{"location":"Tutorials/1_compute_log_determinants_with_stochastic_lanczos_quadrature/#compute-log-determinants-with-stochastic-lanczos-quadrature","title":"Compute log-determinants with stochastic Lanczos quadrature\u00b6","text":"<p>Log-determinant estimation can be implemented with stochastic Lanczos quadrature, which can be loosely interpreted as an extension of Hutchinson's trace estimator.</p>"},{"location":"Tutorials/2_estimate_log_determinants_of_py_tree_valued_functions/","title":"Estimate log-determinants of PyTree-valued functions","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[2]: Copied! <pre>from matfree import decomp, funm, stochtrace\n</pre> from matfree import decomp, funm, stochtrace <p>Create a test-problem: a function that maps a pytree (dict) to a pytree (tuple). Its (regularised) Gauss--Newton Hessian shall be the matrix-vector product whose log-determinant we estimate.</p> In\u00a0[3]: Copied! <pre>def testfunc(x):\n    \"\"\"Map a dictionary to a tuple with some arbitrary values.\"\"\"\n    return jnp.linalg.norm(x[\"weights\"]), x[\"bias\"]\n</pre> def testfunc(x):     \"\"\"Map a dictionary to a tuple with some arbitrary values.\"\"\"     return jnp.linalg.norm(x[\"weights\"]), x[\"bias\"] <p>Create a test-input</p> In\u00a0[4]: Copied! <pre>b = jnp.arange(1.0, 40.0)\nW = jnp.stack([b + 1.0, b + 2.0])\nx0 = {\"weights\": W, \"bias\": b}\n</pre> b = jnp.arange(1.0, 40.0) W = jnp.stack([b + 1.0, b + 2.0]) x0 = {\"weights\": W, \"bias\": b} <p>Linearise the functions</p> In\u00a0[5]: Copied! <pre>f0, jvp = jax.linearize(testfunc, x0)\n_f0, vjp = jax.vjp(testfunc, x0)\nprint(jax.tree.map(jnp.shape, f0))\nprint(jax.tree.map(jnp.shape, jvp(x0)))\nprint(jax.tree.map(jnp.shape, vjp(f0)))\n</pre> f0, jvp = jax.linearize(testfunc, x0) _f0, vjp = jax.vjp(testfunc, x0) print(jax.tree.map(jnp.shape, f0)) print(jax.tree.map(jnp.shape, jvp(x0))) print(jax.tree.map(jnp.shape, vjp(f0))) <pre>((), (39,))\n((), (39,))\n({'bias': (39,), 'weights': (2, 39)},)\n</pre> <p>Use the same API as if the matrix-vector product were array-valued. Matfree flattens all trees internally.</p> In\u00a0[6]: Copied! <pre>def make_matvec(alpha):\n    \"\"\"Create a matrix-vector product function.\"\"\"\n\n    def fun(fx, /):\n        r\"\"\"Matrix-vector product with $J J^\\top + \\alpha I$.\"\"\"\n        vjp_eval = vjp(fx)\n        matvec_eval = jvp(*vjp_eval)\n        return jax.tree.map(lambda x, y: x + alpha * y, matvec_eval, fx)\n\n    return fun\n</pre> def make_matvec(alpha):     \"\"\"Create a matrix-vector product function.\"\"\"      def fun(fx, /):         r\"\"\"Matrix-vector product with $J J^\\top + \\alpha I$.\"\"\"         vjp_eval = vjp(fx)         matvec_eval = jvp(*vjp_eval)         return jax.tree.map(lambda x, y: x + alpha * y, matvec_eval, fx)      return fun In\u00a0[7]: Copied! <pre>matvec = make_matvec(alpha=0.1)\nnum_matvecs = 3\ntridiag_sym = decomp.tridiag_sym(num_matvecs)\nintegrand = funm.integrand_funm_sym_logdet(tridiag_sym)\nsample_fun = stochtrace.sampler_normal(f0, num=10)\nestimator = stochtrace.estimator(integrand, sampler=sample_fun)\nkey = jax.random.PRNGKey(1)\nlogdet = estimator(matvec, key)\nprint(logdet)\n</pre> matvec = make_matvec(alpha=0.1) num_matvecs = 3 tridiag_sym = decomp.tridiag_sym(num_matvecs) integrand = funm.integrand_funm_sym_logdet(tridiag_sym) sample_fun = stochtrace.sampler_normal(f0, num=10) estimator = stochtrace.estimator(integrand, sampler=sample_fun) key = jax.random.PRNGKey(1) logdet = estimator(matvec, key) print(logdet) <pre>4.1267824\n</pre> <p>For reference: flatten all arguments and compute the dense log-determinant:</p> In\u00a0[8]: Copied! <pre>f0_flat, unravel_func_f = jax.flatten_util.ravel_pytree(f0)\n</pre> f0_flat, unravel_func_f = jax.flatten_util.ravel_pytree(f0) In\u00a0[9]: Copied! <pre>def make_matvec_flat(alpha):\n    \"\"\"Create a flattened matrix-vector-product function.\"\"\"\n\n    def fun(f_flat):\n        \"\"\"Evaluate a flattened matrix-vector product.\"\"\"\n        f_unravelled = unravel_func_f(f_flat)\n        vjp_eval = vjp(f_unravelled)\n        matvec_eval = jvp(*vjp_eval)\n        f_eval, _unravel_func = jax.flatten_util.ravel_pytree(matvec_eval)\n        return f_eval + alpha * f_flat\n\n    return fun\n</pre> def make_matvec_flat(alpha):     \"\"\"Create a flattened matrix-vector-product function.\"\"\"      def fun(f_flat):         \"\"\"Evaluate a flattened matrix-vector product.\"\"\"         f_unravelled = unravel_func_f(f_flat)         vjp_eval = vjp(f_unravelled)         matvec_eval = jvp(*vjp_eval)         f_eval, _unravel_func = jax.flatten_util.ravel_pytree(matvec_eval)         return f_eval + alpha * f_flat      return fun In\u00a0[10]: Copied! <pre>matvec_flat = make_matvec_flat(alpha=0.1)\nM = jax.jacfwd(matvec_flat)(f0_flat)\nprint(jnp.linalg.slogdet(M))\n</pre> matvec_flat = make_matvec_flat(alpha=0.1) M = jax.jacfwd(matvec_flat)(f0_flat) print(jnp.linalg.slogdet(M)) <pre>SlogdetResult(sign=Array(1., dtype=float32), logabsdet=Array(3.8124082, dtype=float32))\n</pre>"},{"location":"Tutorials/2_estimate_log_determinants_of_py_tree_valued_functions/#estimate-log-determinants-of-pytree-valued-functions","title":"Estimate log-determinants of PyTree-valued functions\u00b6","text":"<p>Can we compute log-determinants if the matrix-vector products are pytree-valued? Yes, we can. Matfree natively supports PyTrees.</p>"},{"location":"Tutorials/3_implement_uncertainty_quantification_for_trace_estimation/","title":"Implement uncertainty quantification for trace estimation","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[2]: Copied! <pre>from matfree import stochtrace\n</pre> from matfree import stochtrace In\u00a0[3]: Copied! <pre>A = jnp.reshape(jnp.arange(36.0), (6, 6)) / 36\n</pre> A = jnp.reshape(jnp.arange(36.0), (6, 6)) / 36 In\u00a0[4]: Copied! <pre>def matvec(x):\n    \"\"\"Evaluate a matrix-vector product.\"\"\"\n    return A.T @ (A @ x) + x\n</pre> def matvec(x):     \"\"\"Evaluate a matrix-vector product.\"\"\"     return A.T @ (A @ x) + x In\u00a0[5]: Copied! <pre>x_like = jnp.ones((6,))\nnum_samples = 10_000\n</pre> x_like = jnp.ones((6,)) num_samples = 10_000 In\u00a0[6]: Copied! <pre>normal = stochtrace.sampler_normal(x_like, num=num_samples)\nintegrand = stochtrace.integrand_trace()\nintegrand = stochtrace.integrand_wrap_moments(integrand, [1, 2])\nestimator = stochtrace.estimator(integrand, sampler=normal)\nfirst, second = estimator(matvec, jax.random.PRNGKey(1))\n</pre> normal = stochtrace.sampler_normal(x_like, num=num_samples) integrand = stochtrace.integrand_trace() integrand = stochtrace.integrand_wrap_moments(integrand, [1, 2]) estimator = stochtrace.estimator(integrand, sampler=normal) first, second = estimator(matvec, jax.random.PRNGKey(1)) <p>For normally-distributed base-samples, we know that the variance is twice the squared Frobenius norm.</p> In\u00a0[7]: Copied! <pre>print(second - first**2)\nprint(2 * jnp.linalg.norm(A.T @ A + jnp.eye(6), ord=\"fro\") ** 2)\n</pre> print(second - first**2) print(2 * jnp.linalg.norm(A.T @ A + jnp.eye(6), ord=\"fro\") ** 2) <pre>297.37796\n</pre> <pre>321.7863\n</pre> In\u00a0[8]: Copied! <pre>variance = (second - first**2) / num_samples\nprint(variance)\n</pre> variance = (second - first**2) / num_samples print(variance) <pre>0.029737797\n</pre>"},{"location":"Tutorials/3_implement_uncertainty_quantification_for_trace_estimation/#implement-uncertainty-quantification-for-trace-estimation","title":"Implement uncertainty quantification for trace estimation\u00b6","text":"<p>Computing higher moments of trace-estimates can easily be turned into uncertainty quantification.</p>"},{"location":"Tutorials/3_implement_uncertainty_quantification_for_trace_estimation/#higher-moments","title":"Higher moments\u00b6","text":"<p>Trace estimation involves estimating expected values of random variables. Sometimes, second and higher moments of a random variable are interesting.</p>"},{"location":"Tutorials/3_implement_uncertainty_quantification_for_trace_estimation/#uncertainty-quantification","title":"Uncertainty quantification\u00b6","text":"<p>Variance estimation leads to uncertainty quantification: The variance of the estimator is equal to the variance of the random variable divided by the number of samples.</p>"},{"location":"Tutorials/4_combine_trace_estimation_with_control_variates/","title":"Combine trace estimation with control variates","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[2]: Copied! <pre>from matfree import stochtrace\n</pre> from matfree import stochtrace <p>Create a matrix to whose trace/diagonal to approximate.</p> In\u00a0[3]: Copied! <pre>nrows, ncols = 4, 4\nA = jnp.reshape(jnp.arange(1.0, 1.0 + nrows * ncols), (nrows, ncols))\n</pre> nrows, ncols = 4, 4 A = jnp.reshape(jnp.arange(1.0, 1.0 + nrows * ncols), (nrows, ncols)) <p>Set up the sampler.</p> In\u00a0[4]: Copied! <pre>x_like = jnp.ones((ncols,), dtype=float)\nsample_fun = stochtrace.sampler_normal(x_like, num=10_000)\n</pre> x_like = jnp.ones((ncols,), dtype=float) sample_fun = stochtrace.sampler_normal(x_like, num=10_000) <p>First, compute the diagonal.</p> In\u00a0[5]: Copied! <pre>problem = stochtrace.integrand_diagonal()\nestimate = stochtrace.estimator(problem, sample_fun)\ndiagonal_ctrl = estimate(lambda v: A @ v, jax.random.PRNGKey(1))\n</pre> problem = stochtrace.integrand_diagonal() estimate = stochtrace.estimator(problem, sample_fun) diagonal_ctrl = estimate(lambda v: A @ v, jax.random.PRNGKey(1)) <p>Then, compute trace and diagonal jointly using the estimate of the diagonal as a control variate.</p> In\u00a0[6]: Copied! <pre>def matvec_ctrl(v):\n    \"\"\"Evaluate a matrix-vector product with a control variate.\"\"\"\n    return A @ v - diagonal_ctrl * v\n</pre> def matvec_ctrl(v):     \"\"\"Evaluate a matrix-vector product with a control variate.\"\"\"     return A @ v - diagonal_ctrl * v In\u00a0[7]: Copied! <pre>problem = stochtrace.integrand_trace_and_diagonal()\nestimate = stochtrace.estimator(problem, sample_fun)\ntrace_and_diagonal = estimate(matvec_ctrl, jax.random.PRNGKey(2))\ntrace, diagonal = trace_and_diagonal[\"trace\"], trace_and_diagonal[\"diagonal\"]\n</pre> problem = stochtrace.integrand_trace_and_diagonal() estimate = stochtrace.estimator(problem, sample_fun) trace_and_diagonal = estimate(matvec_ctrl, jax.random.PRNGKey(2)) trace, diagonal = trace_and_diagonal[\"trace\"], trace_and_diagonal[\"diagonal\"] <p>We can, of course, compute it without a control variate as well.</p> In\u00a0[8]: Copied! <pre>problem = stochtrace.integrand_trace_and_diagonal()\nestimate = stochtrace.estimator(problem, sample_fun)\ntrace_and_diagonal = estimate(lambda v: A @ v, jax.random.PRNGKey(2))\ntrace_ref, diagonal_ref = trace_and_diagonal[\"trace\"], trace_and_diagonal[\"diagonal\"]\n</pre> problem = stochtrace.integrand_trace_and_diagonal() estimate = stochtrace.estimator(problem, sample_fun) trace_and_diagonal = estimate(lambda v: A @ v, jax.random.PRNGKey(2)) trace_ref, diagonal_ref = trace_and_diagonal[\"trace\"], trace_and_diagonal[\"diagonal\"] <p>Compare the results. First, the diagonal.</p> In\u00a0[9]: Copied! <pre>print(\"True value:\", jnp.diag(A))\nprint(\"Control variate:\", diagonal_ctrl, jnp.linalg.norm(jnp.diag(A) - diagonal_ctrl))\nprint(\"Approximation:\", diagonal_ref, jnp.linalg.norm(jnp.diag(A) - diagonal_ref))\nprint(\n    \"Control-variate approximation:\",\n    diagonal + diagonal_ctrl,\n    jnp.linalg.norm(jnp.diag(A) - diagonal - diagonal_ctrl),\n)\n</pre> print(\"True value:\", jnp.diag(A)) print(\"Control variate:\", diagonal_ctrl, jnp.linalg.norm(jnp.diag(A) - diagonal_ctrl)) print(\"Approximation:\", diagonal_ref, jnp.linalg.norm(jnp.diag(A) - diagonal_ref)) print(     \"Control-variate approximation:\",     diagonal + diagonal_ctrl,     jnp.linalg.norm(jnp.diag(A) - diagonal - diagonal_ctrl), ) <pre>True value: [ 1.  6. 11. 16.]\nControl variate: [ 0.93919766  5.9305882  10.756255   16.017157  ] 0.26119116\nApproximation: [ 0.926166   5.9537234 10.611627  15.773803 ] 0.45781225\nControl-variate approximation: [ 0.943658   5.8008723 10.868535  15.705603 ] 0.3831176\n</pre> <p>Then, the trace.</p> In\u00a0[10]: Copied! <pre>print(\"True value:\", jnp.trace(A))\nprint(\n    \"Control variate:\",\n    jnp.sum(diagonal_ctrl),\n    jnp.abs(jnp.trace(A) - jnp.sum(diagonal_ctrl)),\n)\nprint(\"Approximation:\", trace_ref, jnp.abs(jnp.trace(A) - trace_ref))\nprint(\n    \"Control variate approximation:\",\n    trace + jnp.sum(diagonal_ctrl),\n    jnp.abs(jnp.trace(A) - trace - jnp.sum(diagonal_ctrl)),\n)\n</pre> print(\"True value:\", jnp.trace(A)) print(     \"Control variate:\",     jnp.sum(diagonal_ctrl),     jnp.abs(jnp.trace(A) - jnp.sum(diagonal_ctrl)), ) print(\"Approximation:\", trace_ref, jnp.abs(jnp.trace(A) - trace_ref)) print(     \"Control variate approximation:\",     trace + jnp.sum(diagonal_ctrl),     jnp.abs(jnp.trace(A) - trace - jnp.sum(diagonal_ctrl)), ) <pre>True value: 34.0\nControl variate: 33.643196 0.3568039\nApproximation: 33.265316 0.734684\nControl variate approximation: 33.31867 0.68133163\n</pre>"},{"location":"Tutorials/4_combine_trace_estimation_with_control_variates/#combine-trace-estimation-with-control-variates","title":"Combine trace estimation with control variates\u00b6","text":"<p>Here is how to implement control variates.</p>"},{"location":"Tutorials/5_implement_vector_calculus_in_linear_complexity/","title":"Implement vector calculus in linear complexity","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[2]: Copied! <pre>from matfree import stochtrace\n</pre> from matfree import stochtrace In\u00a0[3]: Copied! <pre>def divergence_dense(vf):\n    \"\"\"Compute the divergence of a vector field.\"\"\"\n\n    def div_fn(x):\n        J = jax.jacfwd(vf)\n        return jnp.trace(J(x))\n\n    return div_fn\n</pre> def divergence_dense(vf):     \"\"\"Compute the divergence of a vector field.\"\"\"      def div_fn(x):         J = jax.jacfwd(vf)         return jnp.trace(J(x))      return div_fn <p>This implementation computes the divergence of a vector field:</p> In\u00a0[4]: Copied! <pre>def fun(x):\n    \"\"\"Evaluate a scalar valued function.\"\"\"\n    return jnp.dot(x, x) ** 2\n</pre> def fun(x):     \"\"\"Evaluate a scalar valued function.\"\"\"     return jnp.dot(x, x) ** 2 In\u00a0[5]: Copied! <pre>x0 = jnp.arange(1.0, 4.0)\ngradient = jax.grad(fun)\nlaplacian = divergence_dense(gradient)\nprint(jax.hessian(fun)(x0))\nprint(laplacian(x0))\n</pre> x0 = jnp.arange(1.0, 4.0) gradient = jax.grad(fun) laplacian = divergence_dense(gradient) print(jax.hessian(fun)(x0)) print(laplacian(x0)) <pre>[[ 64.  16.  24.]\n [ 16.  88.  48.]\n [ 24.  48. 128.]]\n</pre> <pre>280.0\n</pre> <p>But the implementation above requires $O(d^2)$ storage because it evaluates the dense Jacobian. This is problematic for high-dimensional problems.</p> In\u00a0[6]: Copied! <pre>def divergence_matfree(vf, /, *, num):\n    \"\"\"Compute the divergence with Hutchinson's estimator.\"\"\"\n\n    def divergence(k, x):\n        _fx, jvp = jax.linearize(vf, x)\n        integrand_laplacian = stochtrace.integrand_trace()\n        normal = stochtrace.sampler_normal(x, num=num)\n        estimator = stochtrace.estimator(integrand_laplacian, sampler=normal)\n        return estimator(jvp, k)\n\n    return divergence\n</pre> def divergence_matfree(vf, /, *, num):     \"\"\"Compute the divergence with Hutchinson's estimator.\"\"\"      def divergence(k, x):         _fx, jvp = jax.linearize(vf, x)         integrand_laplacian = stochtrace.integrand_trace()         normal = stochtrace.sampler_normal(x, num=num)         estimator = stochtrace.estimator(integrand_laplacian, sampler=normal)         return estimator(jvp, k)      return divergence <ul> <li>The difference to the \"naive\" implementation is that the implicit one does not form dense Jacobians. It requires $O(d)$ memory and $O(d N)$ operations (for $N$ Monte-Carlo samples). For large-scale problems, it may be the only way of computing Laplacians reliably.</li> </ul> In\u00a0[7]: Copied! <pre>laplacian_matfree = divergence_matfree(gradient, num=10_000)\nprint(laplacian(x0))\nprint(laplacian_matfree(jax.random.PRNGKey(1), x0))\n</pre> laplacian_matfree = divergence_matfree(gradient, num=10_000) print(laplacian(x0)) print(laplacian_matfree(jax.random.PRNGKey(1), x0)) <pre>280.0\n</pre> <pre>276.8678\n</pre> <p>In summary, compute matrix-free linear algebra and algorithmic differentiation to implement vector calculus.</p>"},{"location":"Tutorials/5_implement_vector_calculus_in_linear_complexity/#implement-vector-calculus-in-linear-complexity","title":"Implement vector calculus in linear complexity\u00b6","text":"<p>Implementing vector calculus with conventional algorithmic differentiation can be inefficient. For example, computing the divergence of a vector field requires computing the trace of a Jacobian. The divergence of a vector field is important when evaluating Laplacians of scalar functions.</p> <p>Here is how we can implement divergences and Laplacians without forming full Jacobian matrices:</p>"},{"location":"Tutorials/5_implement_vector_calculus_in_linear_complexity/#divergences-and-laplacians","title":"Divergences and Laplacians\u00b6","text":"<p>The divergence of a vector field is the trace of its Jacobian. The conventional implementation would look like this:</p>"},{"location":"Tutorials/5_implement_vector_calculus_in_linear_complexity/#matrix-free-implementation","title":"Matrix-free implementation\u00b6","text":"<p>If we have access to Jacobian-vector products (which we usually do), we can use matrix-free trace estimation to approximate divergences and Laplacians without forming full Jacobians:</p>"},{"location":"Tutorials/5_implement_vector_calculus_in_linear_complexity/#diagonals-of-jacobians","title":"Diagonals of Jacobians\u00b6","text":"<p>If we replace trace estimation with diagonal estimation, we can compute the diagonal of Jacobian matrices in $O(d)$ memory and $O(dN)$ operations.</p>"},{"location":"Tutorials/6_carry_out_stochastic_trace_estimation_with_minimal_memory/","title":"Carry out stochastic trace estimation with minimal memory","text":"In\u00a0[1]: Copied! <pre>import functools\n</pre> import functools In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\n</pre> import jax import jax.numpy as jnp In\u00a0[3]: Copied! <pre>from matfree import stochtrace\n</pre> from matfree import stochtrace In\u00a0[4]: Copied! <pre>nrows = 100  # but imagine nrows=100,000,000,000 instead\nnsamples = 1_000\n</pre> nrows = 100  # but imagine nrows=100,000,000,000 instead nsamples = 1_000 In\u00a0[5]: Copied! <pre>def large_matvec(v):\n    \"\"\"Evaluate a (dummy for a) large matrix-vector product.\"\"\"\n    return 1.2345 * v\n</pre> def large_matvec(v):     \"\"\"Evaluate a (dummy for a) large matrix-vector product.\"\"\"     return 1.2345 * v In\u00a0[6]: Copied! <pre>integrand = stochtrace.integrand_trace()\nx0 = jnp.ones((nrows,))\nsampler = stochtrace.sampler_rademacher(x0, num=nsamples)\nestimate = stochtrace.estimator(integrand, sampler)\n</pre> integrand = stochtrace.integrand_trace() x0 = jnp.ones((nrows,)) sampler = stochtrace.sampler_rademacher(x0, num=nsamples) estimate = stochtrace.estimator(integrand, sampler) In\u00a0[7]: Copied! <pre>key = jax.random.PRNGKey(1)\ntrace = estimate(large_matvec, key)\nprint(trace)\n</pre> key = jax.random.PRNGKey(1) trace = estimate(large_matvec, key) print(trace) <pre>123.44995\n</pre> <p>The above code requires nrows $\\times$ nsamples storage, which is prohibitive for extremely large matrices. Instead, we can loop around estimate() to do the following: The below code requires nrows $\\times$ 1 storage:</p> In\u00a0[8]: Copied! <pre>sampler = stochtrace.sampler_rademacher(x0, num=1)\nestimate = stochtrace.estimator(integrand, sampler)\nestimate = functools.partial(estimate, large_matvec)\n</pre> sampler = stochtrace.sampler_rademacher(x0, num=1) estimate = stochtrace.estimator(integrand, sampler) estimate = functools.partial(estimate, large_matvec) In\u00a0[9]: Copied! <pre>key = jax.random.PRNGKey(2)\nkeys = jax.random.split(key, num=nsamples)\ntraces = jax.lax.map(estimate, keys)\ntrace = jnp.mean(traces)\nprint(trace)\n</pre> key = jax.random.PRNGKey(2) keys = jax.random.split(key, num=nsamples) traces = jax.lax.map(estimate, keys) trace = jnp.mean(traces) print(trace) <pre>123.44996\n</pre> <p>In practice, we often combine both approaches by choosing the largest nsamples (in the first implementation) so that nrows $\\times$ nsamples fits into memory, and handle all samples beyond that via the split-and-map combination.</p> <p>If we reverse-mode differentiate through the sampler, we have to be careful because by default, reverse-mode differentiation stores all intermediate results (and the memory-efficiency of using jax.lax.map is void). To solve this problem, place a jax.checkpoint around the estimator:</p> In\u00a0[10]: Copied! <pre>traces = jax.lax.map(jax.checkpoint(estimate), keys)\ntrace = jnp.mean(traces)\nprint(trace)\n</pre> traces = jax.lax.map(jax.checkpoint(estimate), keys) trace = jnp.mean(traces) print(trace) <pre>123.44996\n</pre> <p>This implementation recomputes the forward pass for each key during the backward pass, but preserves the memory-efficiency on the backward pass.</p> <p>In summary, memory efficiency can be achieved by calling estimators inside jax.lax.map (with or without checkpoints).</p>"},{"location":"Tutorials/6_carry_out_stochastic_trace_estimation_with_minimal_memory/#carry-out-stochastic-trace-estimation-with-minimal-memory","title":"Carry out stochastic trace estimation with minimal memory\u00b6","text":"<p>Matfree's implementation of stochastic trace estimation via Hutchinson's method defaults to computing all Monte-Carlo samples at once, because this is the fastest implementation as long as all samples fit into memory.</p> <p>Some matrix-vector products, however, are so large that we can only store a single sample in memory at once. Here is how we can wrap calls around the trace estimators in such a scenario to save memory.</p>"},{"location":"Tutorials/6_carry_out_stochastic_trace_estimation_with_minimal_memory/#stochastic-trace-estimation","title":"Stochastic trace estimation\u00b6","text":"<p>The conventional setup for estimating the trace of a large matrix would look like this.</p>"},{"location":"Tutorials/7_compute_matrix_functions_without_materializing_large_matrices/","title":"Compute matrix functions without materializing large matrices","text":"In\u00a0[1]: Copied! <pre>import functools\n</pre> import functools In\u00a0[2]: Copied! <pre>import jax\n</pre> import jax In\u00a0[3]: Copied! <pre>from matfree import decomp, funm\n</pre> from matfree import decomp, funm In\u00a0[4]: Copied! <pre>n = 7  # imagine n = 10^5 or larger\n</pre> n = 7  # imagine n = 10^5 or larger In\u00a0[5]: Copied! <pre>key = jax.random.PRNGKey(1)\nkey, subkey = jax.random.split(key, num=2)\nlarge_matrix = jax.random.normal(subkey, shape=(n, n))\n</pre> key = jax.random.PRNGKey(1) key, subkey = jax.random.split(key, num=2) large_matrix = jax.random.normal(subkey, shape=(n, n)) <p>The expected value is computed with jax.scipy.linalg.</p> In\u00a0[6]: Copied! <pre>key, subkey = jax.random.split(key, num=2)\nvector = jax.random.normal(subkey, shape=(n,))\nexpected = jax.scipy.linalg.expm(large_matrix) @ vector\nprint(expected)\n</pre> key, subkey = jax.random.split(key, num=2) vector = jax.random.normal(subkey, shape=(n,)) expected = jax.scipy.linalg.expm(large_matrix) @ vector print(expected) <pre>[  3.7240484 -10.36642    10.205571   -4.89606    -4.2590394 -19.160347\n -10.71674  ]\n</pre> <p>Instead of using jax.scipy.linalg, we can use matrix-vector products in combination with the Arnoldi iteration to approximate the matrix-function-vector product.</p> In\u00a0[7]: Copied! <pre>def large_matvec(v):\n    \"\"\"Evaluate a matrix-vector product.\"\"\"\n    return large_matrix @ v\n</pre> def large_matvec(v):     \"\"\"Evaluate a matrix-vector product.\"\"\"     return large_matrix @ v In\u00a0[8]: Copied! <pre>num_matvecs = 5\narnoldi = decomp.hessenberg(num_matvecs, reortho=\"full\")\ndense_funm = funm.dense_funm_pade_exp()\nmatfun_vec = funm.funm_arnoldi(dense_funm, arnoldi)\nreceived = matfun_vec(large_matvec, vector)\nprint(received)\n</pre> num_matvecs = 5 arnoldi = decomp.hessenberg(num_matvecs, reortho=\"full\") dense_funm = funm.dense_funm_pade_exp() matfun_vec = funm.funm_arnoldi(dense_funm, arnoldi) received = matfun_vec(large_matvec, vector) print(received) <pre>[  3.872463   -9.765792   10.424392   -5.4151964  -4.4425015 -19.06055\n -10.98663  ]\n</pre> <p>The matrix-function vector product can be combined with all usual JAX transformations. For example, after fixing the matvec-function as the first argument, we can vectorize the matrix function with jax.vmap and compile it with jax.jit.</p> In\u00a0[9]: Copied! <pre>matfun_vec = functools.partial(matfun_vec, large_matvec)\nkey, subkey = jax.random.split(key, num=2)\nvector_batch = jax.random.normal(subkey, shape=(5, n))  # a batch of 5 vectors\nreceived = jax.jit(jax.vmap(matfun_vec))(vector_batch)\nprint(received.shape)\n</pre> matfun_vec = functools.partial(matfun_vec, large_matvec) key, subkey = jax.random.split(key, num=2) vector_batch = jax.random.normal(subkey, shape=(5, n))  # a batch of 5 vectors received = jax.jit(jax.vmap(matfun_vec))(vector_batch) print(received.shape) <pre>(5, 7)\n</pre> <p>Talking about function transformations: we can also reverse-mode-differentiate the matrix functions efficiently.</p> In\u00a0[10]: Copied! <pre>jac = jax.jacrev(matfun_vec)(vector)\nprint(jac)\n</pre> jac = jax.jacrev(matfun_vec)(vector) print(jac) <pre>[[ 0.897139   -0.02348357 -0.68018055 -1.720187    0.27199066  1.6036288\n  -0.11475003]\n [-3.3453507   3.0401454  -2.3894792   3.1318164   5.7527194   2.937097\n   2.9685025 ]\n [ 3.0593514  -3.5127666   2.1038098  -1.3352946  -4.219495   -2.6193018\n  -4.5912113 ]\n [-2.128035    0.62076664 -1.822557    2.5717206   2.2454846   0.36229706\n   1.7297201 ]\n [-1.7773656   1.3572419  -2.3025572   1.2953173   3.2735302   2.3572621\n   2.6441014 ]\n [-5.8010287   5.587347   -3.9821222   4.1583195   9.996261    6.182467\n   8.151909  ]\n [-2.467562    3.2210727  -2.5758777   3.1784697   5.0637693   1.2434095\n   3.6850958 ]]\n</pre> <p>Under the hood, reverse-mode derivatives of Arnoldi- and Lanczos-based matrix functions use the fast algorithm for gradients of the Lanczos and Arnoldi iterations from this paper. Please consider citing it if you use reverse-mode derivatives functions of matrices (a BibTex is here).</p>"},{"location":"Tutorials/7_compute_matrix_functions_without_materializing_large_matrices/#compute-matrix-functions-without-materializing-large-matrices","title":"Compute matrix functions without materializing large matrices\u00b6","text":"<p>Sometimes, we need to compute matrix exponentials, log-determinants, or similar functions of matrices, but our matrices are too big to use functions from scipy.linalg or jax.scipy.linalg. However, matrix-free linear algebra scales to even the largest of matrices. Here is how to use Matfree to compute functions of large matrices.</p>"}]}